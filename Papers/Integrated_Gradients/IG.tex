\documentclass[serif]{beamer} 
\usetheme{Madrid}
\usepackage{relsize}
\usepackage{hyperref}
\usepackage{pbox}
\usepackage{algorithm} 
\usepackage{algorithmic}
\usepackage{mathtools}

%%%%%%%%%%%%%  1  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{ Integrated gradients }
\subtitle{Axiomatic Attribution for Deep Networks\\ Mukund Sundararajan, Ankur Taly, Qiqi Yan }
\author{Ronen Rojas}
\centering
\date{01.11.2021}

\begin{document}
\maketitle

%%%%%%%%%%%%%  2  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{About the paper}
\begin{itemize}
	\item Cited 1868 (Google Scholar) 
	\item 8 sections
	\begin{enumerate}
		\item Definition and motivation
		\item 2 axioms 
		\item Integrated gradients (IG)
		\item IG uniqueness 
		\item IG implementation
		\item IG applications
		\item Related work
		\item Conclusions
	\end{enumerate}
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  3  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 1 - Motivation}
\framesubtitle{Definition of contributions}
\begin{itemize}
    \item Function (deep network) $F: \mathbb{R}^n \rightarrow [0, ~1]$
    \item Attribution $x$ relative to a baseline $x^\prime$ 
	\item $A_F(x, x^\prime)= (a_1, ... , a_n) \in R^n$
	\item Choice of baseline is important:
	\begin{itemize}
			\item Black Image
			\item Zero vector embedding 
	\end{itemize}
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  4  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}{Section 2 - Axioms}
	\framesubtitle{Summary}
	\begin{itemize}
		\item 2 fundemuntals Axioms
		\begin{enumerate}
			\item \textit{Sensitivity}
			\item \textit{Implementation Invariance}
		\end{enumerate}
		
		\item Method mentioned that break it.
		\begin{itemize}
			\item Deconvolutional networks (DeConvNets) - break \textit{Sensitivity}
			\item Guided back-propagation - break \textit{Sensitivity}
			\item DeepLift - break \textit{Implementation Invariance}
			\item Layer-wise relevance (LRP) - break \textit{Implementation Invariance}
		\end{itemize}
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  5  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}{Section 2 - Axioms}		
	\begin{block}{Axiom 1 - \textit{Sensitivity}}
		An attribution method satisfies Sensitivity(a) if for every
		input and baseline that differ in one feature but have different
		predictions then the differing feature should be given
		a non-zero attribution
	\end{block}	
	\begin{itemize}
		\item Gradients are natural analog of the model coefficients
		\item Starting point for attribution method
		\item Break Sensitivity - DeConvNets, Guided back-propagation
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  6  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 2 - Axioms}
	\begin{block}{Simple example}
		\begin{align*}
			f(x) &= 1-\text{ReLU}(1-x) \\
			x&=2,~x^\prime=0\\
			f(0) &= 0, ~f(2) = 1\\
			 f'(2) &= 0
		\end{align*}
	\end{block}
	\begin{itemize} 
	\item DeepLift and LRP tackle the Sensitivity - baseline.
	\item “Discrete gradients” instead of (instantaneous) gradients
	\end{itemize} 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  7  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 2 - Axioms}	
	\begin{block}{Axiom 2 - Implementation Invariance}
		Two networks are functionally equivalent if their outputs
		are equal for all inputs, despite having very different implementations.
		Attribution methods should satisfy Implementation
		Invariance, i.e., the attributions are always identical
		for two functionally equivalent networks
	\end{block}	
	Example for functionally equivalent:
	\begin{itemize}
		\item Network architecture has more degrees freedom to represent a function
		\item Training process can converge to either sets of weights
		\item Attributions should be the same
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  8  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 2 - Implementation Invariance}	
	\begin{itemize}
		\item Gradients are invariant to implementation
		\item $f$ output, $g$ input.
		\item $\frac{\partial f}{\partial g} = \frac{\partial f}{\partial h} \cdot \frac{\partial h}{\partial g}$ , $h$ is implementation dependent (detail)
		\item LRP and DeepLift uses discrete gradients
		\item $\frac{f(x_1)-f(x_0)}{g(x_1)-g(x_0)} \ne \frac{f(x_1)-f(x_0)}{h(x_1)-h(x_0)} \cdot \frac{h(x_1)-h(x_0)}{g(x_1)-g(x_0)}$
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  9  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 3 - Integrated Gradients}
	\begin{block}{In a nutshell}
		IG = $\overbrace{\{LRP, DeepLift\}}^{Sensitivity }$ + \{Implementation Invariance\}
	\end{block}	
	
	\begin{block}{Formal Defintion}
		$a_F^i = IG_i(x) = (x_i-x_i^\prime) \cdot \int_{\alpha=0}^{1} \frac{\partial F(x^\prime + \alpha(x-x^\prime))}{\partial x_i} d \alpha$
	\end{block}	

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  10  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 3 - Integrated Gradients}
	\framesubtitle{Axiom: Completeness}
	\begin{block}{Defintion}
		If F is differentiable almost everywhere then:
		$\sum_{i=1}^n IG_i(x) = F(x) - F(x^\prime)$
	\end{block}	
	\begin{itemize}
		\item Sanity check the attribution.
		\item Fundamental theorem of calculus for path integrals.
		\item Completeness $\Rightarrow $ Sensitivity.
		\item IG - satisfy Implementation Invariance.
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  11  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 4 - Uniqueness}
	\framesubtitle{therein lies the rub}
	\begin{itemize}
		\item Every empirical evaluation technique didn't work
		\item Couldn't delineate data error, a misbehaving model, and a misbehaving attribution method
		\item This explains the axiomatic approach 
		\item IG isn't unique but somewhat canonical 
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%  12  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 4 - Uniqueness}
	\framesubtitle{Path methods}
	\begin{block}{Setup}
		$\gamma = (\gamma_1, ..., \gamma_n) : [0,1] \mapsto \mathbb{R}^n  $ \\
		 $ \gamma(0) = x^\prime, \gamma(1) = x$
	\end{block}	
	\begin{block}{Definition}
		$\text{PathIntegratedGrads}_i^\gamma (x) :: = 	\int_{\alpha=0}^{1} \frac{\partial F(\gamma(\alpha))}{\partial \gamma_i(\alpha)} \cdot  \frac{\partial \gamma_i(\alpha)}{\partial \alpha} d \alpha$
	\end{block}	
	\begin{itemize}
		\item Satisfy Implementation Invariance and Completeness.
	\end{itemize}
\end{frame}


%%%%%%%%%%%%%  13  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 4 - Uniqueness}
	\framesubtitle{Axioms}
	\begin{block}{Sensitivity(b)}
		If the function implemented by the deep network does not depend (mathematically) on some variable, then the attribution to that variable is always zero
	\end{block}	
	\begin{block}{Proposition}
		Path methods
		are the only attribution methods that always satisfy
		Implementation Invariance, Sensitivity(b), Linearity, and
		Completeness.
	\end{block}	
	\begin{itemize}
		\item Integrated gradients correspond to a cost-sharing method called Aumann-Shapley
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Section 4 - Uniqueness}
	\begin{block}{Symmetry-Preserving}
		Two input variables are symmetric w.r.t. a function if swapping them does not change the function. 
	\end{block}	
	\begin{block}{Symmetry-Preserving attribution }
		An  attribution method is symmetry preserving, if for all inputsthat have identical values for symmetric variables and baselines
	that have identical values for symmetric variables, the
	symmetric variables receive identical attributions.
	\end{block}	
	\begin{block}{Proposition}
		Integrated gradients is the unique path method that is symmetry-preserving.
	\end{block}	
\end{frame}

\begin{frame}{Section 5 - Implementation}
	\begin{itemize}
		\item Near zero predication on baseline 
		\item Baseline convey a complete absence of signal.
		\item $\text{IntegratedGrad}_i^{approx}(x)::= (x_i - x_i^\prime) \cdot \sum_{k=1}^m \frac{\partial F(x^\prime + \frac{k}{m}(x-x^\prime))}{\partial x_i}  \cdot \frac{1}{m}$
		\item Use sanity check to tune $m$
	\end{itemize}
\end{frame}

\begin{frame}{Section 6 - Applications}
	\begin{enumerate}
		\item Object Recognition Network - Vision
		\item Diabetic Retinopathy Prediction - Vision
		\item Question Classification -NLP
		\item Neural Machine Translation -NLP
		\item Chemistry Models
	\end{enumerate}
\end{frame}


\end{document}

