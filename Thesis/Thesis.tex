\documentclass[12pt]{report}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{5}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{bbm}
\usepackage{indentfirst}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{braket}
\usepackage{tikz}
\usetikzlibrary{quantikz}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{MnSymbol,wasysym}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{braket}
\usepackage{booktabs}
\graphicspath{ {images/}}
\usepackage[style=numeric]{biblatex}
\addbibresource{thesis.bib}




\title{
	{Lab report - Interpretability evaluation framework: \\ Aspects in Machine learning and hydrology domain applications}\\
	{\large School of Computer Science and Engineering - 
		Institute of Earth Sciences}\\
	{\large Hebrew University} \\
	
}
\author{ 
	{Ronen Rojas}
}
\date{September 2023}


\begin{document}

\maketitle
\tableofcontents
\newpage


\newpage
\chapter{Introduction} 

In recent years, machine learning models have become the standard tool for almost every task of estimation, prediction, and classification. Machine learning models learn for themselves what the important features in the data are by an optimization process called training. This training process has replaced the tedious task of manually extracting insights from the data .\\

The immense progress in tools, software packages, and hardware enhancements has made machine learning widely accessible. All that is needed is a strong computational framework and good, clean data. The performance of these kinds of machine learning efforts is unprecedented and they outperform humans on various tasks (\cite{he2015deep}, \cite{DBLP:journals/corr/abs-1805-01890}, \cite{DBLP:journals/corr/abs-1905-01392}, \cite{DBLP:journals/corr/MnihKSGAWR13}, \cite{Silver_2016}).\\

However, with great power comes great responsibility (as they say). How do we explain a model prediction in a sensible manner? Most machine learning models are considered to be \textit{"black boxes"} and humanly incomprehensible. When considering a trained machine learning model we are now interested more in "why" it predicted something and less in "what" it predicted. This shift has given rise to significant developments in the field of interpretability methods (\cite{molnar2019}, \cite{electronics10050593}, \cite{DBLP:journals/corr/abs-2012-14261}, \cite{DBLP:journals/corr/abs-1802-00614}, \cite{DBLP:journals/corr/abs-2003-07631}, \cite{DBLP:journals/corr/abs-2007-15911}, \cite{samek2019explainable}, \cite{DBLP:journals/corr/abs-2012-15445}). \\

Interpretability tackles the issue of trying to gain more insight of the prediction mechanism of the model. It also tries to answer the following questions:
\begin{itemize}
	\item \textit{Why did the model made the prediction?}	
	\item \textit{Do we have a human-friendly explanation for this prediction?}	
	\item \textit{Can we visualize this explanation? }	
\end{itemize}

Some interpretability methods try to tackle the idea of "why?" instead of "what?" from an axiomatic approach (\cite{DBLP:journals/corr/SundararajanTY17}, \cite{DBLP:journals/corr/abs-2111-07668}, \cite{DBLP:journals/corr/MontavonSM17}, \cite{https://doi.org/10.48550/arxiv.1711.00867}). Some do so from a visualization perspective (\cite{DBLP:journals/corr/abs-1802-00614}, \cite{DBLP:journals/corr/ZhouKLOT15}, \cite{DBLP:journals/corr/ZeilerF13}, \cite{DBLP:journals/corr/SelvarajuDVCPB16}). Even though visualization results can be insightful to humans, such evaluations are not objective and might contain biases.\\ 

There are evaluation frameworks that try to quantify how good an interpretability method is (\cite{DBLP:journals/corr/abs-1912-01451}, \cite{https://doi.org/10.48550/arxiv.1509.06321} \cite{https://doi.org/10.48550/arxiv.1806.10758} \cite{DBLP:journals/corr/MontavonSM17},\cite{LRP},  \cite{DBLP:journals/corr/ArrasHMMS16a}). But since evaluation of interpretability methods can get convoluted and untrustworthy, it is difficult to define a good explanation. As there is no ground truth for that, there is no right or wrong answer to this conundrum. \\ 

Evaluation of interpretability methods is the main motivation for this research. In this paper I will detail methods for refinement and exploration of better understanding our interpretability methods and their limitations. I will evaluate the interpretability methods in a concrete manner and apply them to the scientific domain of hydrology.


\newpage

\chapter{Literature review} 

In this chapter I will review interpretability methods that have gained popularity in recent years. I will also review frameworks of evaluating interpretability methods and what evaluation metrics are used. I will constrain the scope of this work only to model attribution-based explanations \cite{electronics10050593}, these kind of explanations are local and post-hoc i.e., given a trained model and an input for the prediction, attribution measures or ranks each input feature to explain the model's prediction. \\

\section{What is an attribute?}
Formally, if we have a model that was trained to perform a specific task:
\begin{align*}
	y & = f_{\theta}(x) \\
	x & \in \mathbb{R}^n, y \in \mathbb{R}
\end{align*}

An attribution method provides an explanation $E$ for the input $x$:
\begin{align*}
	E &= \text{attrib}(f_{\theta}, x) \\
	x & \in \mathbb{R}^n, E \in \mathbb{R}^n
\end{align*}

$E$ gives an explanation power rank for each input coordinate for the prediction $y=f_{\theta}(x)$. Each coordinate of the input for a model is usually called a feature. 



\newpage
\section{Interpretability methods}

For this section we'll assume we have a model $y=f(x)$ and an attribution-based interpretability  method that outputs an explanation $E$ for the prediction $y$ with the same dimensions as the input $x$.

\subsection{Grad}
	The most straight forward interpretability method \cite{baehrens2010explain} \cite{https://doi.org/10.48550/arxiv.1312.6034}, also known as saliency map:
\begin{align*}
	E_\text{GRAD}(f, x)  = \nabla_{x}f(x)
\end{align*}


\subsection{Input $\times$ Gradient}
Saliency map can be substantially improved \cite{DBLP:journals/corr/ShrikumarGSK16} by simply multiplying the gradient with the input, this is basically first-order Taylor approximation in a sense:
\begin{align*}
	E_{\text{INPUT} \times \text{GRAD}}(f, x)  = x \odot \nabla_{x}f(x)
\end{align*}


\subsection{Integrated Gradients}
Sundararajan et al. \cite{DBLP:journals/corr/SundararajanTY17} proposed this interpretability method, abbreviated IG, denote the coordinate $i$ as of input $x$ as $x_i$: 
\begin{align*}
	[E_\text{IG}(f, x)]_i  = (x_i - \overline{x}_i) \times \int_{\alpha=0}^{\alpha=1}\frac{\partial f(x+ \alpha(\overline{x}-x ))}{\partial x_i} d \alpha
\end{align*}

where $\overline{x}$ is the baseline input for the method, usually something neutral in the data-set e.g. the black image in images data-sets.

\subsection{Local Integrated Gradients}
Ancona et al. \cite{https://doi.org/10.48550/arxiv.1711.06104} suggested a variant of any method that has an multiplication term by the input. If the aforementioned term is omitted then it's call local method whereas the original is called global, in case of the IG method we'll get:
\begin{align*}
	[E_\text{local-IG}(f, x)]_i  = \int_{\alpha=0}^{\alpha=1}\frac{\partial f(x+ \alpha(\overline{x}-x ))}{\partial x_i} d \alpha
\end{align*}

According to \cite{https://doi.org/10.48550/arxiv.1711.06104} \textit{global attribution methods} describe  marginal effect of a feature on the output
with respect to a baseline, whereas  \textit{local attribution methods} describes changes for infinitesimally small perturbations around the original input.\\

\subsection{Smooth Grad}

Smilkov et al. \cite{DBLP:journals/corr/SmilkovTKVW17} proposed a adding some noise to the attribution process that will help smooth the saliency maps :
\begin{align*}
	E_\text{SGRAD}(f, x)  = \frac{1}{N}\sum_{i=1}^{N}\nabla_{x}f(x+g_i)
\end{align*}

Where $g_i$ are noise vectors $g_i \sim \mathcal{N}(0, \sigma^2) $ are drawn i.i.d. from a normal distribution. One can generalize this method, given an attribution method $E(f, x)$ its smooth counterpart is:
\begin{align*}
	E_\text{Smooth}(f, x)  = \frac{1}{N}\sum_{i=1}^{N}E(f, x + g_i)
\end{align*}


\subsection{VarGrad}
Adebayo et al. \cite{DBLP:journals/corr/abs-1810-03307} suggests using the variance operator similarly to the SGRAD. obviously this method can be generalized to any attribution method, denote the variance operator $\mathcal{V}$:
\begin{align*}
	E_\text{Variance}(f, x)  = \mathcal{V} \big[E(f, x + g_i)\big]
\end{align*}


\subsection{Smooth Grad Square SG-SQ}
Hooker et al. \cite{https://doi.org/10.48550/arxiv.1806.10758} proposed the method element-wise square of SGrad 
\begin{align*}
	E_\text{SG-SQ}(f, x)  = E_\text{SGRAD}(f, x) \odot E_\text{SGRAD}(f, x)
\end{align*}

Where $\odot$ is the element-wise product.


\subsection{Shapely Value Sampling}
Shapley values provide is a concept in game theory to calculate "fair" distribution of winnings for each player members of the winning coalition in a cooperative game by measuring marginal contribution to the final outcome (\cite{CASTRO20091726}, \cite{trumbelj2010AnEE} \cite{Shapley+2016+307+318}, \cite{https://doi.org/10.48550/arxiv.2104.12199}, \cite{https://doi.org/10.48550/arxiv.1903.10464}).\\
		
Denote $\mathcal{M}, $ the set of players i.e. the grand coalition, $\mathcal{S}$ the set of players in the partial coalition and $v$ is the contribution map between subsets of players, the Shapley value $\phi_i$ for coalition member $i$ is defined as follows:
\begin{align*}
	\phi_i(v)=  \sum_{\mathcal{S} \subseteq \mathcal{M} \setminus \{i\}} {|\mathcal{M}| \choose  1, |\mathcal{S}|, |\mathcal{M}| -|\mathcal{S}| -1}^{-1}\Big(v(\mathcal{S}\cup \{i\}) - v(\mathcal{S}) \Big)
\end{align*}
	
Based on this definition an efficient  perturbation process for an attribution-based explanation can be devised \cite{strumbelj2010efficient}. By taking a random permutation of the input features $x_i, i \in [N]$, adding them one-by-one to the given baseline $x'$ and defining $v=f(x)- f(x')$. Repeating this process $n$ times, each time choosing a new random permutation of the input features will yield to an empiric estimation of the real Shapley value.


\subsection{LRP}
Layer-wise relevance propagation abbreviated LRP \cite{LRP} \cite{DBLP:journals/corr/MontavonSM17}, is a method that tries to estimate every neurons' layer relevancy. The model output score represents the initial relevance which then is being propagated backwards in a set of pre-defined rules the are applied sequentially to all layers of the model. When the propagation reaches the first layer we will get the wanted attribution of the input. This backward propagation mechanism resembles somewhat of a numerical differentiation \cite{DBLP:journals/corr/abs-2012-14261}:  

\begin{align*}
	[E_\text{LRP}(f, x)]_i  =  x_i \cdot \frac{\partial^{g}f(x)}{\partial x_i}, \quad g(z) = \frac{\sigma(z)}{z}
\end{align*}
where $\sigma(z)$ is the non-linearity in the network and $g(z)$ is a replacement the derivative of $\sigma(z)$.
	
\subsection{DeepLift}
Shrikumar et al. \cite{DBLP:journals/corr/ShrikumarGK17} tried to devise a more intricate backward propagation mechanism called Deep Learning Important FeaTures, DeepLift in short. \\

The mechanism used pre-defined rules for each type of non-linearity and layer type. Their novelty was introducing a reference point in their propagation mechanism.  
\begin{align*}
	[E_\text{DeepLift}(f, x)]_i  =  (x_i-x_i^{ref}) \cdot \frac{\partial^{g}f(x)}{\partial x_i}, \quad g(z) = \frac{\sigma(z)-\sigma(z^{ref})}{z-z^{ref}}
\end{align*}

Both LRP and DeepLIFT have gained the nickname  \textit{discrete gradient methods} \cite{DBLP:journals/corr/abs-2012-14261}.

\subsection{Feature Ablation}
A perturbation based approach to computing attribution, replacing each input feature $x_i$ with a given baseline $x'_i$, and computing the difference in output. 

\subsection{Deconvolution}

Deconvolution \cite{DBLP:journals/corr/ZeilerF13} computes the gradient of the target output with respect to the input, but instead of using the gradients of ReLU functions it is computed taking ReLU of the output gradient, essentially only propagating non-negative gradients \cite{10.1007/978-3-319-46466-4_8}.


\subsection{Guided backpropagation}

Guided backpropagation is a slight varient of the Deconvolution method \cite{https://doi.org/10.48550/arxiv.1412.6806}. The difference is mainly how the gradients of non-linearties are  being calculated. The method adds an additional guidance signal from the higher layers to the usual backpropagation, this in turn prevents backward flow of negative gradients.

\subsection{CAM}

Class Activation Mapping \cite{DBLP:journals/corr/ZhouKLOT15}, CAM in short, is an attribution method which tries to estimate the importance of each unit in the feature maps i.e. the rectified result of the convolutional by using global average pooling (GAP) in CNNs.\\

Since the feature maps have a lower resolution the activation maps are up-sampled, usually the last convolutional is the one that is considered for the attribution.

\subsection{Guided Grad CAM}

Gradient-weighted Class Activation Mapping \cite{DBLP:journals/corr/SelvarajuDVCPB16}, in short Guided Grad CAM, combines two methods.
Guided Grad CAM Computes element-wise product of guided backpropagation attributions with upsampled (non-negative) GradCAM attributions:
\begin{align*}
	E_\text{Guided Grad CAM}(f, x)  = E_\text{Guided Backprop}(f, x) \odot E_\text{CAM}(f, x)
\end{align*}

\subsection{Lime}

Local Interpretable Model-agnostic Explanations \cite{DBLP:journals/corr/RibeiroSG16} (LIME). This method trains an interpretable surrogate model around the input example and using model evaluations at these points to train a simpler interpretable ‘surrogate’ model, for example a linear model.

\newpage

\section{Interpretability methods frameworks and metrics}

\subsection{Feature importance removal framework}

There has been a line of study to treat attribution-based explanations as feature importance (\cite{https://doi.org/10.48550/arxiv.1509.06321} \cite{https://doi.org/10.48550/arxiv.1806.10758} \cite{DBLP:journals/corr/MontavonSM17},\cite{LRP},  \cite{DBLP:journals/corr/ArrasHMMS16a}, \cite{DBLP:journals/corr/abs-1911-03429}, \cite{DBLP:journals/corr/abs-1905-04610}, \cite{DBLP:journals/corr/abs-1904-11829}, \cite{nguyen-2018-comparing}).  This line of works asks what will happen if we "remove" these features from the input strategically?  what will happen to model predictions?, if we remove from most to least important, ranked from the explanations) we will probably see degradation. This can be evaluated by a certain metric. On the other we can ask the opposite question what will happen if we remove the least important features ? can the model sustain it's merits ? until what percentage of the input is removed ? \\

By comparing the degradation or robustness of a certain model we can compare different interpretability method attributes, if explanations from one method "broke" the model quicker in a sense this method is better and vice versa if we were able to sustain accuracy or other performance metrics of a model when removing least important features from explanations ranks. Motavon et al. \cite{DBLP:journals/corr/MontavonSM17} coined this desired property as \textit{Explanation Selectivity}.\\

Samec et al. \cite{https://doi.org/10.48550/arxiv.1509.06321} have devised an input perturbation process called \textit{most relevant first}, abbreviated MoRF for the trained model $f(x)$. In this process we remove information from the input by setting the coordinates (or a region of surrounding pixels e.g. $9 \times 9$ neighborhood) to a uniform distribution. MoRF process is done by the individual attribution ranking $e$ of a given input $x$, this is done for $L$ iterations:
\begin{align*}
	x_{\text{MoRF}}^{(0)} & = x  \\ 
	\forall 1 \le k \le L \quad x_{\text{MoRF}}^{(k)} &= \text{Remove} (x_{\text{MoRF}}^{(k-1)}, e)	
\end{align*}
The metric used is the area over the MoRF perturbation curve (AOPC):
\begin{align*}
	\text{AOPC} = \Braket{	\sum_{k=0}^{L}f(x) -f(x_{\text{MoRF}}^{(k)})}_{p(x)}
\end{align*}
Where $\Braket{\cdot}_{p(x)}$ means we average over all inputs in the data set, a larger AOPC means a sharper decrease of the MoRF process ($f(x_{\text{MoRF}}^{(k)})$) thus a better feature importance for the attribution-based explanation. \\

The opposite approach then is the perturbation process \textit{least relevant first}, abbreviated LeRF. The proposed metric is the area between th perturbations curves:
\begin{align*}
	\text{ABPC} = \Braket{	\sum_{k=0}^{L}f(x_{\text{LeRF}}^{(k)}) -f(x_{\text{MoRF}}^{(k)})}_{p(x)}
\end{align*}
In this case the LeRF process needs to sustain the information as much a possibles thus \textbf{larger} area is wanted.\\

In \cite{DBLP:journals/corr/abs-2003-07631} and \cite{LRP} to evaluate and compare different interpretability method "pixel-flipping" was used.  Arras et el. \cite{DBLP:journals/corr/ArrasHMMS16a} also used LRP evaluation and simply "deleted" word for their model into by setting the corresponding word embedding to zero in order, Lundberg et al. using different strategies on how to remove the input features \cite{DBLP:journals/corr/abs-1905-04610} for evaluations explanations for Tree-based machine learning models. \\

Hooker et al.  \cite{https://doi.org/10.48550/arxiv.1806.10758} opine that any removal of information from the input is disruptive since the metrics will be used on modified input that is not from the data set original distribution, thus the generalization errors of "out of sample" inputs for the model becomes dominant. \\

To mitigate this case \cite{https://doi.org/10.48550/arxiv.1806.10758} presented a training strategy, RemOve and Retrain (ROAR). This ensures that the metric used, which in the case was a simple test-set accuracy, is used in sample from the distrubution the model was trained on. The opposite approach was to Keep And Retrain (KAR). In this approach, \textbf{minimizing} degradation to test-set accuracy is desirable\\

DeYoung et al. \cite{DBLP:journals/corr/abs-1911-03429} used a similar approach in NLP explanations, called rationales in the NLP jargon, evaluation benchmark. \cite{DBLP:journals/corr/abs-1911-03429} proposed ERASER, which stands for \textbf{E}valuation \textbf{R}ationales \textbf{A}nd \textbf{S}imple \textbf{E}nglish \textbf{R}easoning, a framework that utilized the metric of AOPC (among others metrics) with the only exception that re-training is not necessary since the input size for the model (BERT \cite{devlin-etal-2019-bert}) can vary so removing tokens (input features) can be done seamlessly.  \\

Chefer et al. \cite{DBLP:journals/corr/abs-2012-09838} used the ERASER framework for the NLP domain and the "removing" pixels for the vision domain in their evaluation of interpretability methods of self-attentions transformers. In both cases, the metric used was the area-under-the-curve (AUC) perturbation process. 

\newpage
\subsection{Interpretability as a debug tool framework}

Another line of study is to let the interpretability method to be a debug tool for the model or training process itself (\cite{https://doi.org/10.48550/arxiv.1810.03292}, \cite{DBLP:journals/corr/abs-2011-05429}). If an interpretability method is sensitive to a model that trained or tested in an ill manner compared to a model that was trained in a nominal way it means that it can better explain the model and in a way could be a sanity check for the interpretability method itself. \\

Adebayo et al. \cite{https://doi.org/10.48550/arxiv.1810.03292} proposed 2 test frameworks: 
\begin{enumerate}
	\item \textit{Model parameter randomization test}: Apply attributions-based method on a trained model and on the same architecture but on a randomly initialized untrained model.
	\item \textit{Data randomization test}: Apply attributions-based method on a trained model and on a copy of the mode trained on a copy of the data set in which all labels were randomly permuted.
\end{enumerate}

\textit{Model parameter randomization} tests whether attributions method outputs differ substantially between the 2 models, if the output are similar it means the method is insensitive to model parameters which in a sense does not bode well to the explanation goals which are to understand why did the model made its prediction. It's important to note that the randomization of the model parameters was done in a cascading fashion from top to bottom layers. \\

\textit{Data randomization} tests again as before the difference between the 2 models, An insensitivity to the permuted labels reveals that the method does not depend on the relationship between input and labels which is not a desired property for an attribution-based method. \\

\cite{https://doi.org/10.48550/arxiv.1810.03292} used visualization to show the the differences in the attributes, but also used more rigorous ways namely \textit{similarity metrics} as follows:
\begin{enumerate}
	\item Spearman rank correlation with / without  absolute value.
	\item Structural similarity index measure (SSIM \cite{1284395}).
	\item Pearson correlation of the histogram of gradients (HOGs)
\end{enumerate}

In his later work Abedeyo et al. \cite{DBLP:journals/corr/abs-2011-05429} used a more elaborate scheme for tampering with the data-set, the training or evaluation process itself, simply named "bugs":
\begin{enumerate}
	\item \textit{Data contamination bugs} 
	\begin{enumerate}
		\item \textit{Labeling errors} - Incorrectly labeled data , Similar to previous work.
		\item \textit{Spurious Correlation} - Make the model associate uncorrelated reason to the task, e.g. blue sky backgrounds with the bird class. 
	\end{enumerate}
	\item \textit{Model contamination bugs} - re-initialization of model weights, similar to previous work.
	\item \textit{Test-time contamination bugs} - Out of distribution (OOD) samples, domain shift for the data-set. 
\end{enumerate}

\textit{Spurious bug implementation} was implemented by placing all birds onto one of the sky backgrounds and all dogs onto a bamboo forest background. Logically, explanations on a model that was trained on a data-set like this would identify this correlation i.e. attributing most of the background pixels to the class prediction. In a sense this is a ground truth for the explanation output itself and a "debug" tool for the attribution method.\\

\textit{Test-time contamination bugs} assess the ability of attributions to diagnose domain shift, e.g. the attribution for an MNIST input from a model trained on MNIST, to an attribution for the same input but derived from an output of a model trained on a different data-set. \\


The metric used for the attributes is SSIM to compare the similarity between the explanation in the aforementioned tests.\\

\newpage


\subsection{Human subject study}

Incorporating  human study for evaluation of interpretability  methods is a very natural (\cite{DBLP:journals/corr/abs-2011-05429}, \cite{DBLP:journals/corr/abs-2005-01831}, \cite{DBLP:journals/corr/RibeiroSG16}).  The explanations coming out of these methods are meant to make black-box machine learning in a sense more understandable and human comprehensible. \\

Adebayo et al. \cite{DBLP:journals/corr/abs-2011-05429} conduct a 54-person study to assess whether end-users can recognize the bugs in the tests according to the attributions . Evidently this approach showed the people are more biased towards the model predictions even in the presence of its attributions. \\

Hase et al. \cite{DBLP:journals/corr/abs-2005-01831} tried to test \textit{simulatability} on interpretability methods for machine learning models. A model is simulatable when a person can predict
its behavior on new inputs. For this task \cite{DBLP:journals/corr/abs-2005-01831}  conducted a two-fold 39-person human-subject study:
\begin{enumerate}
	\item \textit{Forward simulation} - given an input and an “explanation,” users must predict what a model would output for the given input 
	\item \textit{Counterfactual simulation} - are given an input, a model’s output for that input, and an “explanation” of that output, and then they must predict what the model will output when given a perturbation of the original input.
\end{enumerate}

\newpage
\subsection{Axiomatic approach framework}

In this section we'll present the approach of desired properties which attribution-based interpretability ought to have. These properties are mostly common sense and stem from a more theoretical thought process when devising a scheme for an attribution method hence we call them \textit{axioms} in this context. \\ 

Although these \textit{axioms} are not comparable between methods per se they're still worth to mentioned as it can a used as a rigorous framework to evaluate these methods and some can become more quantifiable in a sense as we'll explain later in this section. \\

\subsubsection{Sensitivity, implementation invariance, linearity and Symmetry preserving}

Sundararajan et al. \cite{DBLP:journals/corr/SundararajanTY17} introduced these axioms in his novel attributed-based method \textit{integrated gradients}:
\begin{itemize}
	\item \textit{Sensitivity} -  If the function implemented by the deep network does not depend (mathematically) on some variable, then the attribution to that variable is always zero.
	\item \textit{Implementation invariance} - Attributions should be identical for two functionally equivalent networks.
	\item \textit{Linearity} - Attributions method should preserve any linearity within the network.
	\item \textit{Symmetry preserving} - If 2 inputs to the network are symmetrical $F(x,y)=F(y,x)$, so should be their corresponding attributions. 
\end{itemize}

\subsubsection{Continuity}
Montavon et al \cite{DBLP:journals/corr/MontavonSM17} introduced the desired axiomatic property of \textit{continuity} as follows:
\begin{itemize}
	\item \textit{Continuity} - If two data points are nearly equivalent, then the explanations of their predictions should also be	nearly equivalent.
\end{itemize}

This axiom can be also quantified as follows, denote an attribution method $E$, two inputs $x,x'$ and their explanations respectively $E,E'$ :
\begin{align*}
	\underset{x \ne x'}{\text{max}} \frac{\| E-E'\|_1}{\| x-x'\|_2}
\end{align*}

Bhatt et al.  \cite{DBLP:journals/corr/abs-2005-00631}  used a similar definition but used the term \textit{low/average sensitivity}
\subsubsection{Input invariance}

Kindermans et al. \cite{https://doi.org/10.48550/arxiv.1711.00867} introduced the \textit{Input invariance} axiom:
\begin{itemize}
	\item \textit{Input invariance} - The attribution-based method should mirror the sensitivity of the model with respect to transformations of the input
\end{itemize}

For example a constant shift in the input with two model that were trained on the original data and the shifted data and have the \textbf{same predictions} should have the \textbf{same attributions}. \\

\subsubsection{Monotonicity}

Although \textit{monotonicity} defined as metric in \cite{DBLP:journals/corr/abs-2007-07584} it stems from an axiomatic approach on the mechanism of attribution methods that assign an importance value to each feature. From a more theoretical approach a more concrete metric was proposed namely \textit{monotonicity}. \\

Nguyen et al. \cite{DBLP:journals/corr/abs-2007-07584}  posit that feature importance rank attributions should follow a desired property, denote the explanation $E_i$ for feature $i \in [N], y^* = f(\mathbf{x}^*)$:
\begin{align*}
	|E_i| \propto \mathbb{E}(l(y^*, f_i)|\mathbf{x}^*_{-i}) = \int_{\mathcal{X}_i} l(y^*, f_i(x_i)p(x_i) d x_i
\end{align*}

Where $l$ is the loss function, density probability function for feature $x_i$ is $p(x_i)$ and $f_i$ is the restriction of the function $f$ to the feature $i$ obtained by fixing the other features at the values $\mathbf{x}^{*}_{-i} = (x_1, ..., x_{i-1}, x_{i+1} , ... , x_{N})$, so the monotonicity metric for attribution $E$ is defined as the Spearman’s
correlation coefficient $\rho_{S}$:
\begin{align*}
	&\mathbf{e} = [|E_1|, ..., |E_i|,..., |E_N|] \\
	&\mathbf{f}  =  [\mathbb{E}(l(y^*, f_1)|\mathbf{x}^*_{-1}), ..., \mathbb{E}(l(y^*, f_i)|\mathbf{x}^*_{-i}) ,..., \mathbb{E}(l(y^*, f_N)|\mathbf{x}^*_{-N})] \\
	&\text{monotonicity}(E,f, \mathbf{x}^*,y^*)   = \rho_{S}(\mathbf{e}, \mathbf{f})
\end{align*}


\subsubsection{Completeness and sensitivity-n}
Sundararajan et al. \cite{DBLP:journals/corr/SundararajanTY17} proved that the attribution-based method \textit{itegrated gradients} holds the axiom of \textit{completeness}:
\begin{align*}
	&\sum_{i=1}^N IG(\mathbf{x}) = f(\mathbf{x})-f(\mathbf{x}')
\end{align*}

Where $\mathbf{x}'$ is a baseline input that is used for the attribution calculation and we'll be elaborated in later sections.  

Ancona et al. \cite{https://doi.org/10.48550/arxiv.1711.06104} generalized the \textit{completeness} axioms namely, \textit{sensitivity-n}:
\begin{itemize}
	\item \textit{sensitivity-n}: For any subset of features of cardinality $n$ , $\mathbf{x}_s= [x_1, ...,x_n] \subseteq \mathbf{x} $ it holds $\sum_{i=1}^n E_i = f(\mathbf{x})- f(\mathbf{x}_{[\mathbf{x}_S =0]})$ 
\end{itemize}

It easy to see that \textit{completeness} axiom is a private case of the \textit{sensitivity-n} axiom when $n=N$. Bhatt et al. \\

\cite{DBLP:journals/corr/abs-2005-00631}  introduced a similar notion of \textit{faithfulness} where the sum of the attributions and the difference in output when setting those features to a reference baseline should be proportionality correlated

\newpage

\subsection{Adversarial framework}

Adversarial framework is a well researched subject for finding vulnerabilities in neural networks performance. Adversarial framework for interpretability methods asks whether the method at hand can be fooled via some sort of manipulation. Although some adversarial techniques did not quantify numerically how an interpretability method is susceptible to a specific attack it is rather important to mention this framework as they give some notion of robustness to the methods.\\

Heo et al. \cite{DBLP:journals/corr/abs-1902-02041} showed that LRP, Grad-CAM, and simple saliency map , can be easily fooled with model manipulation. They also suggested a quantitative metric , \textit{Fooling Success Rate} (FSR). FSR measures how much an interpretability method is prone to aforementioned adversarial manipulation.\\


Slack et al. \cite{DBLP:journals/corr/abs-1911-02508} proposed a novel scaffolding technique that hides classifier biases by allowing an adversarial entity to craft an arbitrary desired explanation for the LIME and methods. Dombrowski et al. \cite{https://doi.org/10.48550/arxiv.1906.07983} exploited certain geometrical properties of neural
networks to manipulate an explanation arbitrarily with hardly perceptible differences in the input.

\newpage
\section{Recognized gap}

Many frameworks tackle tasks like images classification and NLP tasks. They also use popular widely researched existing data sets. I recognized a twofold gap:
\begin{enumerate}
	\item There is no attempt to extract more than the perceived feature importance of the data and model prediction in a method
	\item There is no attempt to utilize and evaluate interpretability methods in hydrological setup.
\end{enumerate}

In a sense I want to know whether the interpretability method was able to gain some more insights from the data, for instance some hidden modalities in the data and labels that are more easily perceived with the explanations rather then the data or can be learned with fewer samples. \\

The incentive of using hydrological domain, is that sometimes the explanation of a trained model can reveal a pattern that even expert judgment could not. Also incorporating machine learning models in general is sparse so it is rather interesting to explore and evaluate the use of these models for regression task in hydrology. \\

\newpage
\chapter{Research objective } 

In this report I will present a novel framework for evaluating an interpretability method. In this framework I will try to see if inside the explanations there is more than meets eyes. \\

I will ingrain the data set with different sets of scenarios that adhere to a general mechanism, this is the way to generate the data-label distribution. But also the process will insure that the different scenarios will create different characteristics that are not so noticeable in the data itself but rather apparent in their counterpart explanations. I will quantify the contrast between these different point of views. \\

These scenarios characteristics can be perceived differently in different method of attributes. it can be easily compared by devising a classification tasks for the scenarios and compare attribution methods.\\

Taking advantage of the fact that the attributes of model and the inputs for the model have the same dimensions, one can devise two type classifiers that have almost the same architecture. One type that uses the real data and one that uses to explanations. It can be stated that by comparing the two types of the classifiers is comparing  apples to apples so to speak. \\

The aforementioned evaluation approach can be summarized in the following scheme \ref{scheme}:
\begin{enumerate}
	\item Generate a 2-modal data-set
	\item Train a machine learning model on the data-set
	\item`Train two classifiers for the modalities of the data-set:
	\begin{enumerate}
		\item A clean view – training only with the data-set
		\item An interpreted view – training only with attributes of the trained model 	
    \end{enumerate}
	\item Evaluate interpretability method by comparison of the clean classifier to interpreted classifier
	with simple classification metrics		
\end{enumerate}
\begin{figure}[H]
	\centering\includegraphics[width=16.4cm]{scheme.png}
	\caption{Scheme}
	\label{scheme}
\end{figure}
\chapter{Data and Models} 

\section{Synthetic Data}

Our machine learning model task is to predict the overground runoff from synthetic weather data for precipitation and potential evapotranspiration (PET) inputs. The creation of the data is done in two phases:

\begin{enumerate}
	\item Weather Generator (Inputs) - daily time series of the precipitation $\Big[\frac{mm}{day}\Big]$ and PET $\Big[\frac{mm}{day}\Big]$.
	\item DREAM (labels) - daily time series of runoff $\Big[\frac{mm}{day}\Big]$.
\end{enumerate}
 
\subsection{Weather Generator}
The weather generator is a essentially a Markov chain $\{X_i\}_{i=0}^n, X_i \in \{0,1\}$ where:
\begin{align*}
	X_{i|i-1} \sim \begin{cases}
		\text{Bernoulli}(\delta_1) & X_{i-1} = 0\\
		\text{Bernoulli}(\delta_2) & X_{i-1} = 1\\
	\end{cases} 
\end{align*}

The precipitation daily values are:
\begin{align*}
	Y_i = X_i \times Z_i
\end{align*} 

Where $Z_i \overset{i.i.d}{\sim} \text{Weibull}(\lambda, k)$ and its density function is defined as follows: 
\begin{equation*}
	f(x; \lambda, k) = \begin{cases}
		\frac{k}{\lambda} \left(\frac{x}{\lambda}\right)^{k-1} e^{-(x/\lambda)^k}, & x \geq 0 \\
		0, & x < 0
	\end{cases}
\end{equation*} 

The PET values are drawn $\overset{i.i.d}{\sim} \mathcal{N}(\mu, \sigma^2)$.\\


For the two modalities of the data set we use different distribution parameters  mode 1 (dry) and mode 2 (wet):
\begin{table}[ht]
	\centering
	\caption{Weather generation parameters}
	\begin{tabular}{lcc}
		\toprule
		\textbf{Parameter} & \textbf{Mode 1} & \textbf{Mode 2} \\
		\midrule
		$\delta_1$ & 0.2 & 0.8 \\
		$\delta_2$ & 0.5 & 0.8 \\
		$k$ & 0.5 & 1.5 \\
		$\mu$ & 3.0 & 3.0 \\
		$\sigma$ & 1.0 & 1.0 \\
		\bottomrule
	\end{tabular}
\end{table}

$\lambda$ was used as a normalization factor to make the annual mean precipitation equal between the two modes. 
\subsection{DREAM}

For the runoff model we've used the well established Hydrometeorological daily recharge assessment model (DREAM) \cite{DREAM}. In short this model tries to delineate between the runoff discharge and the ground water recharge, this model has displayed good empirical results.
\newpage
\section{Models}
\subsection{Feed Forward Neural Network}

We've used Deep Neural Networks (DNNs) for classification tasks. The network consist of multiple layers of interconnected nodes. In the context of classification, DNNs can automatically learn to extract relevant patterns and representations from complex data.\\   
\begin{figure}[H]\centering\includegraphics[width=4cm]{FFNN.png}\caption{Neural Network Architecture}\end{figure}

Every layer is an affine layer that ends with a non-linear point wise activation function e.g.: sigmoid, Relu.\\

An Affine layer is defined as follows: 
\begin{align*}
	y = Ax + b , x \in \mathbb{R}^n, b, y \in \mathbb{R}^m, A \in \mathbb{R}^{m \times n}
\end{align*}

Activation functions:
\begin{itemize}
	\item $\sigma_g(z) = \frac{1}{1+e^{-z}}$ - Sigmoid 
	\item $ReLU(z) = \max(0, z)$ - Rectified linear unit
	\item $\sigma_g(z) = \tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$ - hyperbolic tangent function 
\end{itemize}	
\begin{figure}[H]\centering\includegraphics[width=6cm]{sigmoid.png}\caption{Sigmoid function}\end{figure}
\begin{figure}[H]\centering\includegraphics[width=6cm]{RELU.png}\caption{ReLU function}\end{figure}
\begin{figure}[H]\centering\includegraphics[width=6cm]{tanh.png}\caption{Hyperbolic tangent function}\end{figure}



The process of training involves adjusting the network's parameters through backpropagation, where errors are propagated backward to fine-tune the connections between nodes.\\

There are different loss function for different tasks, MSE, L1, Cross entropy. But for the classification task we've used the binary cross-entropy loss:
\begin{align*}
	l(x,y) = 	-x\log(y)-(1-x)\log(1-y)
\end{align*}
 


\subsection{The Support Vector Classifier (SVC)}

The Support Vector Classifier (SVC) aims to find the optimal hyperplane that separates classes in a binary classification problem. One can use a linear kernel to find the hyperplane separator but some non-linear kernels $\phi(x)$ can be utilized to get a non-linear separation.\\   

Given a set of training samples $\{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}$ where $x_i$ is the feature vector and $y_i \in \{-1, 1\}$ is the class label, the optimization problem for SVM with a kernel function:


\begin{align*}
	& \underset{w, b, \xi}{\text{min}}
	& & \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i \\
	& \text{s.t.}
	& & y_i (w \cdot \phi(x_i) + b) \geq 1 - \xi_i, \quad i = 1, \ldots, n \\
	& & & \xi_i \geq 0, \quad i = 1, \ldots, n
\end{align*}


where:
\begin{align*}
	&w  \text{ is the weight vector perpendicular to the hyperplane}, \\
	&b  \text{ is the bias term}, \\
	&\xi_i  \text{ are slack variables}, \\
	&C  \text{ is the regularization parameter}, \\
	&\phi(x_i)  \text{ is the feature mapping of } x_i \text{ into a higher-dimensional space using the kernel function }.
\end{align*}

\newpage
\subsection{RNN}

\subsubsection{RNN - Introduction}

Recurrent Neural Networks (RNNs) have connections that loop back, this enables the network to have memory of past inputs so to speak. The input to RNNs are signals with timestamp (time series) - $\{x_t, y_t \}_{t=1}^k$.\\

RNNS utilis the hidden state to predict the labels $h_t = f_{W}(x_t, h_{t-1})$.\\
\begin{figure}[H]\centering\includegraphics[width=10cm]{RNN.png}\caption{RNN}\end{figure}


However, standard RNNs can suffer from vanishing or exploding gradients, limiting their ability to capture long-range dependencies. To overcome these issues, variants like Long Short-Term Memory (LSTM) were developed which will be presented in the next section as it will be our main workhorse for the hydrology modeling.
\newpage

\subsection{LSTM }

LSTM is a special kind of RNN, designed to overcome the short-comes of RNN such as complicated training procedures and  difficulty to processes long sequences. The architecture of LSTM networks facilitates (as its eponymous name) the learning of both short-term and long-term patterns, remembering information for long periods of time is intrinsic to LSTM.

\subsubsection{LSTM - Principles}

LSTMs has three gating mechanisms: the input gate, forget gate, and output gate. These gates enable selective retention or discard of information over time.\\

Each gate is essentially an Affine transformation followed by a non-linear point wise function:
\begin{align*}
	f_t &= \sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\
	i_t &= \sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\
	o_t &= \sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\
	\tilde{c}_t &= \sigma_h(W_{c} x_t + U_{c} h_{t-1} + b_c) \\
	c_t &= f_t \circ c_{t-1} + i_t \circ \tilde{c}_t \\
	h_t &= o_t \circ \sigma_h(c_t)
\end{align*}

where: 
\begin{align*}
	&x_t \in \mathbb{R}^{d}: \text{input vector to the LSTM unit}\\
	&f_t \in \mathbb{R}^{h}: \text{forget gate's activation vector}\\
	&i_t \in \mathbb{R}^{h}: \text{input/store gate's activation vector}\\
	&o_t \in \mathbb{R}^{h}: \text{output gate's activation vector}\\
	&h_t \in \mathbb{R}^{h}: \text{hidden state vector also known as output vector of the LSTM unit}\\
	&\tilde{c}_t \in \mathbb{R}^{h} : \text{cell input activation vector}\\
	&c_t \in \mathbb{R}^{h} : \text{cell state vector}\\
	& \sigma_g: \text{sigmoid function} \\
	& \sigma_h:  \text{hyperbolic tangent function}
\end{align*}
The initial values are $c_0 = 0$ and $h_0 = 0$ and the operator $\circ$ denotes the Hadamard product (element-wise product). 


\begin{figure}[H]\centering\includegraphics[width=7.5cm]{LSTM_MAIN.jpeg}\caption{LSTM - Scheme}\end{figure}

\begin{figure}[H]\centering\includegraphics[width=14cm]{LSTM_SCHEME.jpeg}\caption{LSTM - Scheme}\end{figure}




\subsubsection{LSTM- Multi-cell}

LSTM Multi-cell architecture stack memory cells in parallel. This enables  having a more expressive networks making them more easy to deal with intricate temporal short-term and long-term dependencies in complex sequences:
\begin{figure}[H]\centering\includegraphics[width=10cm]{LSTM_MULTI.png}\caption{LSTM - Multi layer cells}\end{figure}

\subsubsection{LSTM - in context of rainfall}

LSTM in hydrology have gained popularity in recent years, specifically the success of incorporating LSTM in rainfall-runoff modeling is well established \cite{kratzert2018rainfall}, \cite{kratzert2019toward1},\cite{kratzert2019towards2}  \cite{gauch2021rainfall}.

\textcolor{red}{paragraph on architectures details, NSE loss, performance}\\
%\begin{itemize}
%	\item 2018
%	\begin{itemize}
%		\item CAMELS - Data set
%		\item 2-Layer LSTM -cells
%		\item Input feature size $d=5$ - prcp(mm/day), srad(W/m2), tmax(C), tmin(C), vp(Pa). 
%		\item Hidden state  size $h=20$
%		\item Dropout rate = $10\%$
%		\item Sequemce length - $365$ days. 
%	\end{itemize} 
%	\item 2019
%	\begin{itemize}
%		\item CAMELS - Data set
%		\item 1-Layer LSTM 
%		\item Input feature size $d=5$ ? 
%		\item Hidden state  size $h=256$
%		\item Dropout rate = $40\%$
%		\item Sequence length - $270$ days. 
%	\end{itemize} 	
%\end{itemize}

\newpage


\chapter{Results} 
 
\section{Sanity Check}
\textcolor{red}{paragraph on the rationale of the example}\\

\textcolor{red}{Formulation and figures of the results}\\

\textcolor{red}{paragraph on explaining the results}\\
\newpage
\section{Dream synthetic Framework}
\subsection{Dream synthetic model}
\textcolor{red}{Figures and explanation of the Rainfall runoff data}\\

\textcolor{red}{Paragraph on how the 2 scenarios are distinguished from each other }\\

\textcolor{red}{Paragraph on how the LSTM was trained, event-driven, MSE and NSE}\\

\textcolor{red}{Figures and explanation of several events prediction Vs. label}\\
\subsection{Interpretability method}

\textcolor{red}{Paragraph on how what methods were used and what were the considerations or constraints}\\

\subsection{Framework evaluation results}
\textcolor{red}{Figures and explanation of the metric used, the process in which they were evaluated and the actual results on each method, lots of comparisons!}\\

\chapter{Conclusions} 
\textcolor{red}{TBD}\\

\newpage
	

\printbibliography



\end{document}
