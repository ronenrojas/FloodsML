{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "from typing import Tuple, List\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader, Dataset\n", "import tqdm.notebook\n", "import tqdm\n", "import datetime\n", "import pathlib\n", "import pytz\n", "import seaborn as sns\n", "import os\n", "import copy\n", "import matplotlib.dates as mdates\n", "import json\n", "from LSTM import CNNLSTM\n", "parent_dir = str(Path(os.getcwd()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["RUN_LOCALLY = True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["PATH_ROOT = parent_dir + \"/\"  # Change only here the path\n", "PATH_DATA_FILE = parent_dir + str(Path(\"/\" + \"Data/raw_data_fixed_17532_3_22_38\"))\n", "DIMS_JSON_FILE_PATH = parent_dir + \"./dims_json.json\"\n", "PATH_LABEL = PATH_ROOT + \"Data/CWC/\"\n", "PATH_LOC = PATH_ROOT + \"Data/LatLon/{0}_lat_lon\"\n", "PATH_DATA_CLEAN = PATH_ROOT + \"Data/IMD_Lat_Lon_reduced/\"\n", "PATH_MODEL = PATH_ROOT + \"cnn_lstm/\"\n", "DISCH_FORMAT = \"CWC_discharge_{0}_clean\"\n", "PATH_CATCHMENTS = PATH_ROOT + \"Data/catchments.xlsx\"\n", "FILE_FORMAT = \"data_{0}_{1}\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["LAT_MIN = 17.375\n", "LAT_MAX = 22.625\n", "LON_MIN = 73.625\n", "LON_MAX = 82.875\n", "GRID_DELTA = 0.25\n", "LAT_GRID = np.arange(LAT_MIN, LAT_MAX + GRID_DELTA / 2, GRID_DELTA)\n", "LON_GRID = np.arange(LON_MIN, LON_MAX + GRID_DELTA / 2, GRID_DELTA)\n", "DATA_LEN = 17532\n", "NUM_CHANNELS = 3\n", "DEFAULT_LAT = len(LAT_GRID)\n", "DEFAULT_LON = len(LON_GRID)\n", "DATA_START_DATE = (1967, 1, 1)\n", "DATA_END_DATE = (2014, 12, 31)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_index(data, date_input):\n", "    year, month, day = date_input\n", "    return int(np.where(np.array(data[0] == year) * np.array(data[1] == month) * np.array(data[2] == day))[0].squeeze())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_geo_raw_data(lat, lon, start_date, end_date):\n", "    # Getting data by lat lon coordinates\n", "    data = pd.read_csv(PATH_DATA_CLEAN + FILE_FORMAT.format(lat, lon), header=None, delim_whitespace=True)\n", "    idx_start, idx_end = get_index(data, start_date), get_index(data, end_date)\n", "    x = np.array(data[3][idx_start:idx_end + 1])\n", "    x = np.concatenate([[x], [np.array(data[4][idx_start:idx_end + 1])], [np.array(data[5][idx_start:idx_end + 1])]]).T\n", "    return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_catchment_dict(sheet_path):\n", "    df = pd.read_excel(sheet_path, index_col=0).dropna().T\n", "    means = df.mean().values\n", "    stds = df.std(ddof=0).values\n", "    x = df.values\n", "    catch_dict = {k: x[i, :] for i, k in enumerate(df.T.columns)}\n", "    catch_dict['mean'] = means\n", "    catch_dict['std'] = stds\n", "    return catch_dict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_date_range_and_idx(start_date, end_date, date_range):\n", "    start_date_pd = pd.to_datetime(datetime.datetime(start_date[0], start_date[1], start_date[2], 0, 0))\n", "    end_date_pd = pd.to_datetime(datetime.datetime(end_date[0], end_date[1], end_date[2], 0, 0))\n", "    idx = np.where(np.bitwise_and(start_date_pd <= date_range, date_range <= end_date_pd))[0]\n", "    date_range_out = pd.date_range(start_date_pd, end_date_pd)\n", "    return date_range_out, idx"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_index_by_date(date_in, start_date=DATA_START_DATE, end_date=DATA_END_DATE):\n", "    start_date_pd = pd.to_datetime(datetime.datetime(start_date[0], start_date[1], start_date[2], 0, 0))\n", "    end_date_pd = pd.to_datetime(datetime.datetime(end_date[0], end_date[1], end_date[2], 0, 0))\n", "    date_range = pd.date_range(start_date_pd, end_date_pd)\n", "    date_in_pd = pd.to_datetime(datetime.datetime(date_in[0], date_in[1], date_in[2], 0, 0))\n", "    idx = np.where(date_in_pd == date_range)[0]\n", "    assert len(idx) > 0, f\"Please supply a date between {start_date} and {end_date}\"\n", "    return idx"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_months_by_dates(start_date, end_date):\n", "    start_date_pd = pd.to_datetime(datetime.datetime(start_date[0], start_date[1], start_date[2], 0, 0))\n", "    end_date_pd = pd.to_datetime(datetime.datetime(end_date[0], end_date[1], end_date[2], 0, 0))\n", "    date_range = pd.date_range(start_date_pd, end_date_pd)\n", "    months = [date_range[i].month for i in range(0, len(date_range))]\n", "    return months"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_index_by_lat_lon(lat, lon, lat_grid=LAT_GRID, lon_grid=LON_GRID):\n", "    i = np.where(lat == lat_grid)[0]\n", "    assert len(i) > 0, f\"Please supply latitude between {min(lat_grid)} and {max(lat_grid)}\"\n", "    j = np.where(lon == lon_grid)[0]\n", "    assert len(j) > 0, f\"Please supply longitude between {min(lon_grid)} and {max(lon_grid)}\"\n", "    return i, j"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_index_all_data(date_in, lat, lon, lat_grid=LAT_GRID, lon_grid=LON_GRID, start_date=DATA_START_DATE,\n", "                       end_date=DATA_END_DATE):\n", "    date_i = get_index_by_date(date_in, start_date=DATA_START_DATE, end_date=DATA_END_DATE)\n", "    lat_i, lon_i = get_index_by_lat_lon(lat, lon, lat_grid=LAT_GRID, lon_grid=LON_GRID)\n", "    return date_i, lat_i, lon_i"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_basin_mask(basin, lat_grid, lon_grid):\n", "    df = pd.read_csv(PATH_LOC.format(basin), header=None, delim_whitespace=True)\n", "    basin_lat_lot = df.values\n", "    h = len(lat_grid)\n", "    w = len(lon_grid)\n", "    idx_mat = np.ones((h, w), dtype=bool)\n", "    for lat_lon_i in basin_lat_lot:\n", "        i, j = get_index_by_lat_lon(lat_lon_i[0], lat_lon_i[1], lat_grid, lon_grid)\n", "        idx_mat[i[0], j[0]] = False\n", "    return idx_mat"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_basin_discharge(basin_name, start_date, end_date):\n", "    # Getting Discharge (how much water are running through\n", "    # specific point / river in a given amount of time -\n", "    # usually cubic metre per second)\n", "    data_discharge = pd.read_csv(PATH_LABEL + DISCH_FORMAT.format(basin_name),\n", "                                 header=None, delim_whitespace=True)\n", "    idx_start, idx_end = get_index(data_discharge, start_date), get_index(data_discharge, end_date)\n", "    y = np.array(data_discharge[3][idx_start:idx_end + 1])\n", "    return y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def reshape_data(x: np.ndarray, y: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n", "    \"\"\"\n", "    Reshape matrix data into sample shape for LSTM training.\n", "    :param x: Matrix containing input features column wise and time steps row wise\n", "    :param y: Matrix containing the output feature.\n", "    :param seq_length: Length of look back days for one day of prediction\n", "    :return: Two np.ndarrays, the first of shape (samples, length of sequence,\n", "      number of features), containing the input data for the LSTM. The second\n", "      of shape (samples, 1) containing the expected output for each input\n", "      sample.\n", "    \"\"\"\n", "    num_samples, num_features = x.shape\n", "    x_new = np.zeros((num_samples - seq_length + 1, seq_length, num_features))\n", "    y_new = np.zeros((num_samples - seq_length + 1, 1))\n", "    for i in range(0, x_new.shape[0]):\n", "        x_new[i, :, :num_features] = x[i:i + seq_length, :]\n", "        y_new[i, :] = y[i + seq_length - 1, 0]\n", "    return x_new, y_new"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def reshape_data_basins(x: np.ndarray, y: np.ndarray, seq_length: int, basin_list: list, lead: int) -> Tuple[\n", "    np.ndarray, np.ndarray]:\n", "    \"\"\"\n", "    Reshape matrix data into sample shape for LSTM training.\n", "    :param x: Matrix containing input features column wise and time steps row wise\n", "    :param y: Matrix containing the output feature.\n", "    :param seq_length: Length of look back days for one day of prediction\n", "    :return: Two np.ndarrays, the first of shape (samples, length of sequence,\n", "      number of features), containing the input data for the LSTM. The second\n", "      of shape (samples, 1) containing the expected output for each input\n", "      sample.\n", "    \"\"\"\n", "    n_basins = len(basin_list)\n", "    data_size = int(x.shape[0] / n_basins)\n", "    for i in range(n_basins):\n", "        if i == 0:\n", "            x_new, y_new = reshape_data(x[:data_size - lead, :], y[lead:data_size], seq_length)\n", "        else:\n", "            idx = i * data_size\n", "            x_temp, y_temp = reshape_data(x[idx:idx - lead + data_size, :], y[idx + lead:idx + data_size], seq_length)\n", "            x_new = np.concatenate([x_new, x_temp], axis=0)\n", "            y_new = np.concatenate([y_new, y_temp], axis=0)\n", "    return x_new, y_new"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class IMDGodavari(Dataset):\n", "    \"\"\"\n", "    Torch Dataset for basic use of data from the data set.\n", "    This data set provides meteorological observations and discharge\n", "    of a given basin from the IMD Godavari data set.\n", "    \"\"\"\n", "    def __init__(self, all_data: np.array, basin_list: List, seq_length: int, period: str = None,\n", "                 dates: List = None, months: List = None, min_values: np.array = None, max_values: np.array = None,\n", "                 idx: list = [True, True, True], lead=0, mask_list=[0, 0.5, 0.5], include_static: np.bool = True,\n", "                 mean_y=None, std_y=None):\n", "        \"\"\"Initialize Dataset containing the data of a single basin.\n", "        :param basin_list: List of basins.\n", "        :param seq_length: Length of the time window of meteorological\n", "        input provided for one time step of prediction.\n", "        (currently it's 30 - 30 days)\n", "        :param period: (optional) One of ['train', 'eval']. None loads the entire time series.\n", "        :param dates: (optional) List of the start and end date of the discharge period that is used.\n", "        \"\"\"\n", "        self.basin_list = basin_list\n", "        self.seq_length = seq_length\n", "        self.period = period\n", "        self.dates = dates\n", "        self.months = months\n", "        self.min_values = min_values\n", "        self.max_values = max_values\n", "        self.mean_y = mean_y\n", "        self.std_y = std_y\n", "        self.idx_features = idx\n", "        self.lead = lead\n", "        self.mask_list = mask_list\n", "        self.num_features = None\n", "        self.include_static = include_static\n", "        # load data\n", "        self.x, self.y = self._load_data(all_data)\n", "        # store number of samples as class attribute\n", "        self.num_samples = self.x.shape[0]\n", "        # store number of features as class attribute\n", "    def __len__(self):\n", "        return self.num_samples\n", "    def __getitem__(self, idx: int):\n", "        return self.x[idx], self.y[idx]\n", "    def _load_data(self, all_data):\n", "        \"\"\"Load input and output data from text files.    \"\"\"\n", "        start_date, end_date = self.dates\n", "        idx_s = get_index_by_date(start_date)[0]\n", "        idx_e = get_index_by_date(end_date)[0]\n", "        # Data is reduced to the dates and features sub space.\n", "        # the features are the channels (3 features - precipitation,\n", "        # minimum temperature, maximum temperature)\n", "        data = copy.deepcopy(all_data[idx_s:idx_e + 1, idx_features, :, :])\n", "        time_span = data.shape[0]\n", "        if self.period == 'train':\n", "            # axis = 0 - getting the minimum / maximum of first dimension\n", "            # (\"rows\", but here it's not really rows because it's a tensor and not a matrix)\n", "            # axis = 1 - getting the minimum / maximum of second dimension\n", "            # (\"columns\", but here it's not really columns because it's a tensor and not a matrix)\n", "            # Normalizing the data. There are two typical types of normalization:\n", "            # 1. (min - max normalization) - subtract the min and divide by (max - min)\n", "            # 2. (std - mean normalization) - subtract the mean and divide by the std\n\n", "            # specifically, here we are getting the minimum / maximum\n", "            # of all of the time stamps + H_LAT + W_LON per channel - i.e. - for all of the channels\n", "            # we are calculating the min / max over all the other dimensions.\n", "            self.min_values = data.min(axis=0).min(axis=1).min(axis=1)\n", "            self.max_values = data.max(axis=0).max(axis=1).max(axis=1)\n", "        for i in range(data.shape[1]):\n", "            data[:, i, :] -= self.min_values[i]\n", "            data[:, i, :] /= (self.max_values[i] - self.min_values[i])\n", "        self.num_features = data.shape[2] * data.shape[3]\n", "        for i, basin in enumerate(self.basin_list):\n", "            if i == 0:\n", "                x, x_s = self._get_basin_data(basin, data, start_date, end_date)\n", "                y = get_basin_discharge(basin, start_date, end_date)\n", "                if self.period == 'train':\n", "                    # Scaling the training data for each basin\n", "                    y = self._upadte_basin_dict(basin, y)\n", "            else:\n", "                x_temp, x_s_temp = self._get_basin_data(basin, data, start_date, end_date)\n", "                y_temp = get_basin_discharge(basin, start_date, end_date)\n", "                if self.period == 'train':\n", "                    # Scaling the training data for each basin\n", "                    y_temp = self._upadte_basin_dict(basin, y_temp)\n", "                x = np.concatenate([x, x_temp], axis=0)\n", "                if self.include_static == True:\n", "                    x_s = np.concatenate([x_s, x_s_temp], axis=0)\n", "                y = np.concatenate([y, y_temp])\n", "        if self.include_static == True:\n", "            x = np.concatenate([x, x_s], axis=1)\n", "        else:\n", "            self.num_attributes = 0\n", "        # normalize data, reshape for LSTM training and remove invalid samples\n", "        print(['1: ', x.shape, y.shape], 'Original size')\n", "        x, y = reshape_data_basins(x, np.matrix(y).T, self.seq_length, self.basin_list, self.lead)\n", "        print(['2: ', x.shape, y.shape], 'After reshape and trimming sequenece and lead')\n", "        x, y = self.get_monthly_data(x, y, start_date, end_date)\n", "        print(['3: ', x.shape, y.shape], 'Monthly pick')\n", "        print(\"Data set for {0} for basins: {1}\".format(self.period, self.basin_list))\n", "        print(\"Number of attributes should be: {0}\".format(self.num_attributes))\n", "        print(\"Number of features should be: num_features + num_attributes= {0}\".format(\n", "            self.num_features + self.num_attributes))\n", "        print(\"Number of sample should be: (time_span - sequence_len + 1 -lead) x num_basins= {0}\".format(\n", "            (time_span - self.seq_length + 1 - self.lead) * len(self.basin_list)))\n", "        print(\"Data size for LSTM should be: (num_samples, sequence_len, num_features) = {0}\".format(x.shape))\n", "        # convert arrays to torch tensors\n", "        x = torch.from_numpy(x.astype(np.float32))\n", "        y = torch.from_numpy(y.astype(np.float32))\n", "        return x, y\n", "    def _get_basin_data(self, basin, data, start_date, end_date):\n", "        mask = create_basin_mask(basin, LAT_GRID, LON_GRID)\n", "        x = copy.deepcopy(data)\n", "        for j in range(x.shape[1]):\n", "            x[:, j, mask] = self.mask_list[j]\n", "        x_static_vec = (CATCHMENT_DICT[basin] - CATCHMENT_DICT['mean']) / CATCHMENT_DICT['std']\n", "        # for Efart! duplicating the static features to each of the input images\n", "        x_static = np.repeat([x_static_vec], x.shape[0], axis=0)\n", "        _, self.num_attributes = x_static.shape\n", "        if self.include_static == False:\n", "            self.num_attributes = 0\n", "            x_static = None\n", "        num_features = x.shape[2] * x.shape[3]\n", "        print(['get_basin_data', x.shape[2], x.shape[3]])\n", "        x = np.reshape(x, (x.shape[0], x.shape[1] * num_features))\n", "        return x, x_static\n", "    def local_rescale(self, feature: np.ndarray, variable: str, mean_std=None) -> np.ndarray:\n", "        \"\"\"Rescale output features with local mean/std.\n", "          :param feature: Numpy array containing the feature(s) as matrix.\n", "          param variable: Either 'inputs' or 'output' showing which feature will\n", "          be normalized\n", "        :return: array containing the normalized feature\n", "        \"\"\"\n", "        if mean_std:\n", "            mean_y, std_y = mean_std\n", "            return feature * std_y + mean_y\n", "        n_basins = len(self.basin_list)\n", "        idx = int(len(feature) / n_basins)\n", "        for i, basin_name in enumerate(self.basin_list):\n", "            if basin_name not in self.mean_y.keys():\n", "                raise RuntimeError(\n", "                    f\"Unknown Basin {basin_name}, the trainig data was trained on {list(self.mean_y.keys())}\")\n", "            if i == 0:\n", "                y = feature[i * idx:(i + 1) * idx]\n", "                y = y * self.std_y[basin_name] + self.mean_y[basin_name]\n", "            else:\n", "                y_temp = feature[i * idx:(i + 1) * idx]\n", "                y_temp = y_temp * self.std_y[basin_name] + self.mean_y[basin_name]\n", "                y = np.concatenate([y, y_temp])\n", "        return y\n", "    def _upadte_basin_dict(self, basin_name, y):\n", "        if self.mean_y is None:\n", "            self.mean_y = {}\n", "            self.std_y = {}\n", "        mu_y = y.mean()\n", "        std_y = y.std()\n", "        self.mean_y[basin_name] = mu_y\n", "        self.std_y[basin_name] = std_y\n", "        return (y - mu_y) / std_y\n", "    def get_monthly_data(self, x, y, start_date, end_date):\n", "        if self.months is None:\n", "            return x, y\n", "        else:\n", "            # Rescaling the label\n", "            if self.period == 'train':\n", "                y = self.local_rescale(y, 'output')\n", "            # getting the months for each date\n", "            date_months = get_months_by_dates(start_date, end_date)\n", "            # Adjusting for sequence length and lead\n", "            date_months = date_months[(self.seq_length + self.lead - 1):]\n", "            n_samples_per_basin = int(len(y) / len(self.basin_list))\n", "            ind_date_months = [i for i in range(0, n_samples_per_basin) if date_months[i] in self.months]\n", "            ind_include = []\n", "            for j in range(len(self.basin_list)):\n", "                idx_temp = [idx + j * n_samples_per_basin for idx in ind_date_months]\n", "                if self.period == 'train':\n", "                    y[idx_temp] = self._upadte_basin_dict(self.basin_list[j], y[idx_temp])\n", "                ind_include += idx_temp\n", "            x = x[ind_include, :, :]\n", "            y = y[ind_include]\n", "            return x, y\n", "    def get_min(self):\n", "        return self.min_values\n", "    def get_max(self):\n", "        return self.max_values\n", "    def get_mean_y(self):\n", "        return self.mean_y\n", "    def get_std_y(self):\n", "        return self.std_y\n", "    def get_num_features(self):\n", "        return self.num_features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DNN(nn.Module):\n", "    def __init__(self, input_size: int, num_hidden_layers: int, num_hidden_units: int, dropout_rate: float = 0.0):\n", "        super(DNN, self).__init__()\n", "        self.input_size = input_size\n", "        self.num_hidden_layers = num_hidden_layers\n", "        self.num_hidden_units = num_hidden_units\n", "        self.dropout_rate = dropout_rate\n", "        self.input_layer = nn.Linear(self.input_size, self.num_hidden_units)\n", "        self.hidden_layer = nn.Linear(self.num_hidden_units, self.num_hidden_units)\n", "        self.output_layer = nn.Linear(self.num_hidden_units, 1)\n", "        self.dropout = nn.Dropout(p=self.dropout_rate)\n", "    def forward(self, x):\n", "        batch_size, timesteps, ts_size = x.size()\n", "        x = x.view(batch_size, timesteps * ts_size)\n", "        x = self.input_layer(x)\n", "        for i in range(0, self.num_hidden_layers):\n", "            x = self.hidden_layer(F.relu(self.hidden_layer(x)))\n", "        pred = self.dropout(self.output_layer(x))\n", "        return pred"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_epoch(model, optimizer, loader, loss_func, epoch):\n", "    \"\"\"Train model for a single epoch.\n", "    :param model: A torch.nn.Module implementing the LSTM model\n", "    :param optimizer: One of PyTorchs optimizer classes.\n", "    :param loader: A PyTorch DataLoader, providing the trainings\n", "        data in mini batches.\n", "    :param loss_func: The loss function to minimize.\n", "    :param epoch: The current epoch (int) used for the progress bar\n", "    \"\"\"\n", "    # set model to train mode (important for dropout)\n", "    model.train()\n", "    pbar = tqdm.notebook.tqdm(loader)\n", "    pbar.set_description(f\"Epoch {epoch}\")\n", "    # request mini-batch of data from the loader\n", "    for xs, ys in pbar:\n", "        # delete previously stored gradients from the model\n", "        optimizer.zero_grad()\n", "        # push data to GPU (if available)\n", "        xs, ys = xs.to(device), ys.to(device)\n", "        # get model predictions\n", "        y_hat = model(xs)\n", "        # calculate loss\n", "        loss = loss_func(y_hat, ys)\n", "        # calculate gradients\n", "        loss.backward()\n", "        # update the weights\n", "        optimizer.step()\n", "        # write current loss in the progress bar\n", "        pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_model(model, loader) -> Tuple[torch.Tensor, torch.Tensor]:\n", "    \"\"\"Evaluate the model.\n", "    :param model: A torch.nn.Module implementing the LSTM model\n", "    :param loader: A PyTorch DataLoader, providing the data.\n", "    :return: Two torch Tensors, containing the observations and\n", "        model predictions\n", "    \"\"\"\n", "    # set model to eval mode (important for dropout)\n", "    model.eval()\n", "    obs = []\n", "    preds = []\n", "    # in inference mode, we don't need to store intermediate steps for\n", "    # backprob\n", "    with torch.no_grad():\n", "        # request mini-batch of data from the loader\n", "        for xs, ys in loader:\n", "            # push data to GPU (if available)\n", "            xs = xs.to(device)\n", "            # get model predictions\n", "            y_hat = model(xs)\n", "            obs.append(ys)\n", "            preds.append(y_hat)\n", "    return torch.cat(obs), torch.cat(preds)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_nse(obs: np.array, sim: np.array) -> float:\n", "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n", "    :param obs: Array containing the observations\n", "    :param sim: Array containing the simulations\n", "    :return: NSE value.\n", "    \"\"\"\n", "    # only consider time steps, where observations are available\n", "    # COMMENT FROM EFRAT TO RONEN: NEGATIVE VALUES ARE FINE! I COMMENTED THE TWO LINES BELOW\n", "    # sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n", "    # obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n\n", "    # check for NaNs in observations\n", "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n", "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n", "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n", "    numerator = np.sum((sim - obs) ** 2)\n", "    nse_val = 1 - numerator / denominator\n", "    return nse_val"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_persist_nse(obs: np.array, sim: np.array, lead) -> float:\n", "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n", "    :param obs: Array containing the observations\n", "    :param sim: Array containing the simulations\n", "    :return: NSE value.\n", "    \"\"\"\n", "    # only consider time steps, where observations are available\n", "    # COMMENT FROM EFRAT TO RONEN: NEGATIVE VALUES ARE FINE! I COMMENTED THE TWO LINES BELOW\n", "    # sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n", "    # obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n\n", "    # check for NaNs in observations\n", "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n", "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n\n", "    # the reference is the last observed, instead of the mean\n", "    sim = sim[lead:]\n", "    obs = obs[lead:]\n", "    ref = obs[:-lead]\n", "    denominator = np.sum((obs - ref) ** 2)\n", "    numerator = np.sum((sim - obs) ** 2)\n", "    persist_nse_val = 1 - numerator / denominator\n", "    return persist_nse_val"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_bias(obs: np.array, sim: np.array) -> float:\n", "    \"\"\" Calculate bias\n", "    :param obs: Array containing the observations\n", "    :param sim: Array containing the simulations\n", "    :return: NSE value.\n", "    \"\"\"\n", "    bias_95 = None\n", "    if (np.percentile(obs, 95) != 0):\n", "        bias_95 = (np.percentile(sim, 95) - np.percentile(obs, 95)) / np.percentile(obs, 95) * 100\n", "    bias_5 = None\n", "    if (np.percentile(obs, 5) != 0):\n", "        bias_5 = (np.percentile(sim, 5) - np.percentile(obs, 5)) / np.percentile(obs, 5) * 100\n", "    mean_bias = None\n", "    if (np.nanmean(obs) != 0):\n", "        mean_bias = (np.nanmean(sim) - np.nanmean(obs)) / np.nanmean(obs) * 100\n", "    return bias_95, bias_5, mean_bias"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_maxdif(obs: np.array, sim: np.array) -> float:\n", "    \"\"\" Calculate max difference in percent\n", "    :param obs: Array containing the observations\n", "    :param sim: Array containing the simulations\n", "    :return: maxdif value.\n", "    \"\"\"\n", "    max_sim = np.nanmax(sim)\n", "    max_obs = np.nanmax(obs)\n", "    return (max_sim - max_obs) / max_obs * 100"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_vol_qp(obs: np.array) -> float:\n", "    \"\"\" Calculate volume [10^6 m^3] and peak discharge [m^3/s]\n", "    :param obs: Array containing the observations\n", "    :return: vol and qp values.\n", "    \"\"\"\n", "    vol = np.nansum(obs) * 3600 * 24 / 1E6  # translate from m^3/s in daily resolution to 10^6 m^3\n", "    qp = np.nanmax(obs)\n", "    return vol, qp"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def convert_to_number(number):\n", "    try:\n", "        ret_number = int(number)\n", "        return ret_number\n", "    except Exception:\n", "        return None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Latitude - Width<br>\n", "Longitude - Height"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def reshape_data_by_lat_lon_file(data_file_path, dims_json_file_path):\n", "    lat = DEFAULT_LAT\n", "    lon = DEFAULT_LON\n", "    try:\n", "        f = open(dims_json_file_path)\n", "        dims_json_all = json.load(f)\n", "        if data_file_path in dims_json_all.keys():\n", "            dims_json = dims_json_all[data_file_path]\n", "            if \"Lat\" in dims_json.keys():\n", "                lat = dims_json[\"Lat\"]\n", "            if \"Lon\" in dims_json.keys():\n", "                lon = dims_json[\"Lon\"]\n", "    except Exception as e:\n", "        print(\"There was an exception in reading the dims file: {}\".format(e))\n", "    data_ret = np.fromfile(data_file_path).reshape((DATA_LEN, NUM_CHANNELS, lat, lon))\n", "    return data_ret, lat, lon"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["all_data, LAT, LON = reshape_data_by_lat_lon_file(PATH_DATA_FILE, DIMS_JSON_FILE_PATH)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Number of GPUs available. Use 0 for CPU mode."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ngpu = 1\n", "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n", "# Set random seed for reproducibility\n", "# manualSeed = 999\n", "# #manualSeed = random.randint(1, 10000) # use if you want new results\n", "# #print(\"Random Seed: \", manualSeed)\n", "# random.seed(manualSeed)\n", "# torch.manual_seed(manualSeed)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###############################<br>\n", "#### Meta parameters ##########<br>\n", "###############################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hidden_size = 128  # Number of LSTM cells\n", "dropout_rate = 0.01  # Dropout rate of the final fully connected Layer [0.0, 1.0]\n", "# learning_rate = 2e-3 # Learning rate used to update the weights\n", "learning_rate = 1e-4  # Learning rate used to update the weights\n", "sequence_length = 30  # Length of the meteorological record provided to the network\n", "num_layers = 2  # Number of LSTM cells\n", "lead = 0  # 1\n", "cnn_outputsize = 20\n", "num_hidden_layers = 3\n", "num_hidden_units = 128"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Choose features ###"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["use_perc = True\n", "# maximum temprature in a given day\n", "use_t_max = False\n", "# minimum temprature in a given day\n", "use_t_min = False\n", "idx_features = [use_perc, use_t_max, use_t_min]\n", "### Choose basin ###\n", "# basin_list = ['Mancherial', 'Perur' ,'Pathagudem','Polavaram', 'Tekra']\n", "basin_list = ['Tekra', 'Perur']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############<br>\n", "Data set up#<br>\n", "############"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CATCHMENT_DICT = create_catchment_dict(PATH_CATCHMENTS)\n", "INCLUDE_STATIC = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Training data<br>\n", "start_date = (1967, 1, 1)<br>\n", "end_date = (1999, 12, 31)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start_date = (2000, 1, 1)\n", "end_date = (2009, 12, 31)\n", "months_lst = [6, 7, 8, 9, 10]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Train dataset\\n===============================')\n", "ds_train = IMDGodavari(all_data,\n", "                       basin_list=basin_list,\n", "                       seq_length=sequence_length,\n", "                       period=\"train\",\n", "                       dates=[start_date, end_date],\n", "                       months=months_lst,\n", "                       idx=idx_features,\n", "                       lead=lead,\n", "                       include_static=INCLUDE_STATIC)\n", "tr_loader = DataLoader(ds_train, batch_size=64, shuffle=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test data. We use the feature min/max of the training period for normalization<br>\n", "start_date = (1995, 1, 1)<br>\n", "end_date = (1999, 12, 31)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start_date = (2000, 1, 1)\n", "end_date = (2009, 12, 31)\n", "print('\\nTest dataset\\n===============================')\n", "ds_test = IMDGodavari(all_data, basin_list,\n", "                      seq_length=sequence_length,\n", "                      period=\"eval\",\n", "                      dates=[start_date, end_date],\n", "                      months=months_lst,\n", "                      idx=idx_features, lead=lead,\n", "                      min_values=ds_train.get_min(),\n", "                      max_values=ds_train.get_max(),\n", "                      mean_y=ds_train.get_mean_y(),\n", "                      std_y=ds_train.get_std_y(),\n", "                      include_static=INCLUDE_STATIC)\n", "test_loader = DataLoader(ds_test, batch_size=2048, shuffle=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#######################<br>\n", "Model, Optimizer, Loss#<br>\n", "#######################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we create our model<br>\n", "attributes == static features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_attributes = CATCHMENT_DICT['Tekra'].size"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not INCLUDE_STATIC:\n", "    num_attributes = 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["idx_features - a True / False list over the 3 features (channels) of each \"image\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["input_size = (sum(idx_features) * DEFAULT_LON * DEFAULT_LAT + num_attributes) * sequence_length\n", "input_image_size = (sum(idx_features), LAT, LON)\n", "model = CNNLSTM(lat=LAT, lon=LON, input_size=cnn_outputsize, num_layers=num_layers, hidden_size=hidden_size,\n", "                dropout_rate=dropout_rate, num_channels=sum(idx_features),\n", "                num_attributes=num_attributes, image_input_size=input_image_size).to(device)\n", "# model = DNN(input_size=input_size, num_hidden_layers=num_hidden_layers,\n", "# num_hidden_units=num_hidden_units,\n", "# dropout_rate=dropout_rate).to(device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n", "loss_func = nn.MSELoss()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_epochs = 50  # Number of training epochs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creating the checkpoint folders"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["datetime_israel = datetime.datetime.now(pytz.timezone('Israel'))\n", "path_train_ckpt = PATH_MODEL + datetime_israel.strftime(\"%Y_%m_%d-%H-%M-%S/\")\n", "pathlib.Path(path_train_ckpt).mkdir(parents=True, exist_ok=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range(n_epochs):\n", "    train_epoch(model, optimizer, tr_loader, loss_func, i + 1)\n", "    obs, preds = eval_model(model, test_loader)\n", "    preds = ds_test.local_rescale(preds.cpu().numpy(), variable='output')\n", "    nse = calc_nse(obs.numpy(), preds)\n", "    tqdm.tqdm.write(f\"Test NSE: {nse:.3f}\")\n", "    model_name = \"epoch_{:d}_nse_{:.3f}.ckpt\".format(i + 1, nse)\n", "    torch.save(model, path_train_ckpt + model_name)\n", "    last_model_path = path_train_ckpt + model_name"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate on test set<br>\n", "Validation data. We use the feature means/stds of the training period for normalization<br>\n", "Training data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use existing model<br>\n", "CNNLSTM:<br>\n", "path_to_ckpt = 'drive/MyDrive/RonenRojasEfratMorin/Code/cnn_lstm/2021_08_17-09-30-35/epoch_50_nse_0.798.ckpt'<br>\n", "model = torch.load(path_to_ckpt)<br>\n", "CNNLSTM monsoon<br>\n", "path_to_ckpt = 'drive/MyDrive/RonenRojasEfratMorin/Code/cnn_lstm/2021_08_23-18-29-43/epoch_50_nse_0.588.ckpt'<br>\n", "model = torch.load(path_to_ckpt)<br>\n", "DNN:<br>\n", "path_to_ckpt = 'drive/MyDrive/RonenRojasEfratMorin/Code/cnn_lstm/2021_08_19-15-36-05/epoch_50_nse_0.881.ckpt'<br>\n", "model = torch.load(path_to_ckpt)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["start_date = (2000, 1, 1)<br>\n", "end_date = (2014, 12, 31)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start_date = (2010, 1, 1)\n", "end_date = (2014, 12, 31)\n", "months_lst = [6, 7, 8, 9, 10]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Validation_basin = [\"Tekra\"]\n", "ds_val = IMDGodavari(all_data,\n", "                     basin_list=Validation_basin,\n", "                     seq_length=sequence_length,\n", "                     period=\"eval\",\n", "                     dates=[start_date, end_date],\n", "                     months=months_lst,\n", "                     idx=idx_features,\n", "                     lead=lead,\n", "                     min_values=ds_train.get_min(),\n", "                     max_values=ds_train.get_max(),\n", "                     mean_y=ds_train.get_mean_y(),\n", "                     std_y=ds_train.get_std_y(),\n", "                     include_static=INCLUDE_STATIC)\n", "val_loader = DataLoader(ds_val, batch_size=2048, shuffle=False)\n", "# path_to_ckpt = \"drive/MyDrive/Efrat/cnn_lstm/2021_08_10-14-48-52/epoch_15_nse_0.774.ckpt\"\n", "# path_to_ckpt = last_model_path\n", "# if path_to_ckpt:\n", "#   # 'drive/MyDrive/Efrat/model_lstm/2021_07_24-21-23-40/epoch_6_nse_0.825.ckpt'\n", "#   model = torch.load(path_to_ckpt)\n", "obs, preds = eval_model(model, val_loader)\n", "preds = ds_val.local_rescale(preds.cpu().numpy(), variable='output')\n", "obs = obs.numpy()\n", "nse = calc_nse(obs, preds)\n", "pb95, pb5, total_b = calc_bias(obs, preds)\n", "# Plot results\n", "start_date_tpl = ds_val.dates[0]\n", "start_date = pd.to_datetime(\n", "    datetime.datetime(start_date_tpl[0], start_date_tpl[1], start_date_tpl[2], 0, 0)) + pd.DateOffset(\n", "    days=ds_val.seq_length + ds_val.lead)\n", "end_date_tpl = ds_val.dates[1]\n", "temp = pd.to_datetime(datetime.datetime(end_date_tpl[0], end_date_tpl[1], end_date_tpl[2], 0, 0))\n", "end_date = temp + pd.DateOffset(days=1)\n", "date_range = pd.date_range(start_date, end_date)\n", "# months = get_months_by_dates(start_date, end_date)\n", "ind_include = [i for i in range(0, len(date_range)) if date_range[i].month in months_lst]\n", "date_range = date_range[ind_include]\n", "fig, ax = plt.subplots(figsize=(20, 6))\n", "ax.plot(date_range, obs, label=\"observation\")\n", "ax.plot(date_range, preds, label=\"prediction\")\n", "ax.legend()\n", "ax.set_title(\n", "    f\"Basin {Validation_basin} - Validation set NSE: {nse:.3f}, 95bias: {pb95:.1f}, 5bias: {pb5:.3f} ,total bias : {total_b: .1f}%\")\n", "ax.xaxis.set_tick_params(rotation=90)\n", "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))\n", "ax.set_xlabel(\"Date\")\n", "ax.grid('on')\n", "_ = ax.set_ylabel(\"Discharge (mm/d)\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Integrated gradients\n<br>\n", "# Install package<br>\n", "from captum.attr import IntegratedGradients<br>\n", "# Calculate Integrated Gradients<br>\n", "# path_to_ckpt = last_model_path<br>\n", "# path_to_ckpt = 'drive/MyDrive/RonenRojasEfratMorin/Code/cnn_lstm/2021_08_17-09-30-35/epoch_50_nse_0.798.ckpt'<br>\n", "# if path_to_ckpt:<br>\n", "#   model = torch.load(path_to_ckpt)<br>\n", "start_date_ig = (2012, 8, 26)<br>\n", "end_date_ig = (2012, 9, 5)<br>\n", "# start_date_ig = (start_date.year,start_date.month,start_date.day) # the full validation period<br>\n", "# end_date_ig = (end_date.year,end_date.month,end_date.day)<br>\n", "# new_date_range, idx =  get_date_range_and_idx(start_date_ig, end_date_ig, date_range)<br>\n", "# set model to eval mode (important for dropout)<br>\n", "model.eval()<br>\n", "model.cpu()<br>\n", "ig = IntegratedGradients(model, multiply_by_inputs=True)<br>\n", "basline = torch.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n", "integ_grad = np.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n", "for i in idx:<br>\n", "    integ_grad += ig.attribute(ds_val.x[i:(i + 1), :, :], basline).numpy()<br>\n", "integ_grad = np.squeeze(integ_grad)<br>\n", "integ_grad /= len(idx)<br>\n", "_ = model.cuda()<br>\n", "image_grad = integ_grad[:, :DEFAULT_LAT * DEFAULT_LON].reshape((sequence_length, DEFAULT_LAT, DEFAULT_LON))<br>\n", "time_vector_grad = np.sum(image_grad.reshape((image_grad.shape[0], image_grad.shape[1] * image_grad.shape[2])), axis=1)<br>\n", "spatial_image_grad = np.sum(image_grad, axis=0)<br>\n", "atrrib_grade = integ_grad[:, DEFAULT_LAT * DEFAULT_LON:]<br>\n", "# Calculate Integrated Gradients by quantile<br>\n", "# path_to_ckpt = last_model_path<br>\n", "# path_to_ckpt = 'drive/MyDrive/RonenRojasEfratMorin/Code/cnn_lstm/2021_08_17-09-30-35/epoch_50_nse_0.798.ckpt'<br>\n", "# if path_to_ckpt:<br>\n", "#   model = torch.load(path_to_ckpt)<br>\n", "predsmonsoon = preds[np.where((date_range.month >= 6) & (date_range.month <= 10))[0]]<br>\n", "obsmonsoon = obs[np.where((date_range.month >= 6) & (date_range.month <= 10))[0]]<br>\n", "threshq1 = np.percentile(predsmonsoon, 90)<br>\n", "threshq2 = np.percentile(predsmonsoon, 55)<br>\n", "# idx = np.asarray([i for i in range(0,len(preds)) if (preds[i]>threshq1) & (preds[i]<threshq2)])<br>\n", "idx = np.asarray([i for i in range(0, len(preds)) if (preds[i] > threshq1)])<br>\n", "# idx = np.where((preds>threshq1) & (preds<threshq2) & (date_range.month>=6) & (date_range.month<=10))[0]<br>\n", "print([threshq1, threshq2, idx.shape])<br>\n", "# set model to eval mode (important for dropout)<br>\n", "model.eval()<br>\n", "model.cpu()<br>\n", "ig = IntegratedGradients(model, multiply_by_inputs=True)<br>\n", "basline = torch.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n", "integ_grad = np.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n", "for i in idx:<br>\n", "    # print (i)<br>\n", "    integ_grad += ig.attribute(ds_val.x[i:(i + 1), :, :], basline).numpy()<br>\n", "integ_grad = np.squeeze(integ_grad)<br>\n", "integ_grad /= len(idx)<br>\n", "_ = model.cuda()<br>\n", "image_grad = integ_grad[:, :DEFAULT_LAT * DEFAULT_LON].reshape((sequence_length, DEFAULT_LAT, DEFAULT_LON))<br>\n", "time_vector_grad = np.sum(image_grad.reshape((image_grad.shape[0], image_grad.shape[1] * image_grad.shape[2])), axis=1)<br>\n", "spatial_image_grad = np.sum(image_grad, axis=0)<br>\n", "atrrib_grade = integ_grad[:, DEFAULT_LAT * DEFAULT_LON:]<br>\n", "# integ_file = PATH_ROOT + \"Out/integ_grad_2000_2014\"<br>\n", "# np.save(file=integ_file, arr=integ_grad)<br>\n", "# Plot Integrated Gradients - Spatial<br>\n", "sequence_length_small = 9<br>\n", "image_grad_small = image_grad[sequence_length - sequence_length_small:, :]<br>\n", "n_w_win = 3<br>\n", "n_h_win = int((sequence_length_small + 1) / n_w_win)<br>\n", "fig, ax = plt.subplots(n_h_win, n_w_win, figsize=(10 * n_h_win, 6 * n_w_win))<br>\n", "max_v = abs(image_grad_small).max()<br>\n", "min_v = -max_v<br>\n", "for i in range(sequence_length_small):<br>\n", "    ax.flat[i].set_title(f'Day {i - sequence_length_small}')<br>\n", "    df = pd.DataFrame(image_grad_small[i, :], index=list(LAT_GRID), columns=list(LON_GRID))<br>\n", "    sns.heatmap(df[::-1], ax=ax.flat[i], vmin=min_v, vmax=max_v, square=True, cmap='RdYlBu')<br>\n", "# plot without catchment attributes<br>\n", "with plt.style.context('ggplot'):<br>\n", "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))<br>\n", "    # ax1.plot(new_date_range, obs[idx], label=\"observation\")<br>\n", "    # ax1.plot(new_date_range, preds[idx], label=\"prediction\")<br>\n", "    # ax1.plot(idx, obs[idx], 'x', label=\"observation\")<br>\n", "    # ax1.plot(idx, preds[idx],'x', label=\"prediction\")<br>\n", "    ax1.plot(idx, obs[idx], 'x', label=\"observation\")<br>\n", "    ax1.plot(idx, preds[idx], 'x', label=\"prediction\")<br>\n", "    ax1.legend()<br>\n", "    #  ax1.set_title(f\"Basin {Validation_basin} discharge>q95\")<br>\n", "    ax1.set_title(f\"Basin {Validation_basin}, monsoon, discharge: >q90\")<br>\n", "    # ax1.xaxis.set_tick_params(rotation=90)<br>\n", "    ax1.set_xlabel(\"Date\")<br>\n", "    ax1.grid('on')<br>\n", "    _ = ax1.set_ylabel(\"Discharge (mm/d)\")<br>\n", "    #  df = pd.DataFrame(spatial_image_grad, index =list(LAT_GRID), columns =list(LON_GRID))<br>\n", "    df = pd.DataFrame(image_grad_small[sequence_length_small - 3, :], index=list(LAT_GRID), columns=list(LON_GRID))<br>\n", "    #  vmax=np.max(np.abs(spatial_image_grad.flat))<br>\n", "    vmax = np.max(np.abs(image_grad_small[sequence_length_small - 3, :].flat))<br>\n", "    sns.heatmap(df[::-1], ax=ax2, square=True, cmap='RdYlBu', vmin=-vmax, vmax=vmax)<br>\n", "    txn = np.arange(-sequence_length + 1, 0 + 1)<br>\n", "    ax3.plot(txn, time_vector_grad, '-o', color='c')<br>\n", "    ax3.set_xlabel(\"Day\")<br>\n", "    ax3.set_ylabel(\"Integrated gradients\")<br>\n", "    ax3.set_xlim([-sequence_length + 1, 0])<br>\n", "    ax3.set_xticks(txn)<br>\n", "# plot<br>\n", "with plt.style.context('ggplot'):<br>\n", "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))<br>\n", "    ax1.plot(new_date_range, obs[idx], label=\"observation\")<br>\n", "    ax1.plot(new_date_range, preds[idx], label=\"prediction\")<br>\n", "    ax1.legend()<br>\n", "    ax1.set_title(f\"Basin {Validation_basin}\")<br>\n", "    # ax1.xaxis.set_tick_params(rotation=90)<br>\n", "    ax1.set_xlabel(\"Date\")<br>\n", "    ax1.grid('on')<br>\n", "    _ = ax1.set_ylabel(\"Discharge (mm/d)\")<br>\n", "    ax2.bar(['Precipitation', \"Mean Precipitation\", \"Aridity\", \"Area\", \"Mean elevation\"], sum(att),<br>\n", "            color=['b', 'g', 'g', 'g', 'g'])<br>\n", "    ax2.plot(['Precipitation', \"Mean Precipitation\", \"Aridity\", \"Area\", \"Mean elevation\"], [0, 0, 0, 0, 0], 'k')<br>\n", "    ax2.set_ylabel(\"Attribute sum integrated gradients\")<br>\n", "    txn = np.arange(-sequence_length + 1, 0 + 1)<br>\n", "    ax3.plot(txn, att[:, 0], '-o', color='c')<br>\n", "    ax3.set_xlabel(\"Day\")<br>\n", "    ax3.set_ylabel(\"Integrated gradients\")<br>\n", "    ax3.set_xlim([-sequence_length + 1, 0])<br>\n", "    ax3.set_xticks(txn)<br>\n", "# This cell is for creating the raw data - no need to run this<br>\n", "start_date_pd = pd.to_datetime(datetime.datetime(DATA_START_DATE[0], DATA_START_DATE[1], DATA_START_DATE[2], 0, 0))<br>\n", "end_date_pd = pd.to_datetime(datetime.datetime(DATA_END_DATE[0], DATA_END_DATE[1], DATA_END_DATE[2], 0, 0))<br>\n", "date_range = pd.date_range(start_date_pd, end_date_pd)<br>\n", "num_days = len(date_range)<br>\n", "num_features = 3<br>\n", "h = len(LAT_GRID)<br>\n", "w = len(LON_GRID)<br>\n", "data = np.zeros((num_days, num_features, h, w))<br>\n", "for i, lat_i in enumerate(LAT_GRID):<br>\n", "    for j, lon_j in enumerate(LON_GRID):<br>\n", "        x = get_geo_raw_data(lat_i, lon_j, start_date, end_date)<br>\n", "        data[:, :, i, j] = x<br>\n", "out_path = PATH_ROOT + 'Data/'<br>\n", "data.tofile(out_path + \"raw_data_fixed\" + '_'.join([str(_) for _ in data.shape]))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}