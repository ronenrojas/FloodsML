{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm.notebook\n",
    "import tqdm\n",
    "import datetime\n",
    "import pathlib\n",
    "import pytz\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import json\n",
    "from LSTM import CNNLSTM\n",
    "from preprocess_data import Preprocessor\n",
    "from Godavari import IMDGodavari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = str(Path(os.getcwd()).parent)\n",
    "RUN_LOCALLY = True\n",
    "PATH_ROOT = root_dir + \"/\"  # Change only here the path\n",
    "PATH_DATA_FILE = root_dir + str(Path(\"/\" + \"Data/raw_data_fixed_17532_3_22_38\"))\n",
    "DIMS_JSON_FILE_PATH = root_dir + \"./dims_json.json\"\n",
    "PATH_LABEL = PATH_ROOT + \"Data/CWC/\"\n",
    "PATH_LOC = PATH_ROOT + \"Data/LatLon/{0}_lat_lon\"\n",
    "PATH_DATA_CLEAN = PATH_ROOT + \"Data/IMD_Lat_Lon_reduced/\"\n",
    "PATH_MODEL = PATH_ROOT + \"cnn_lstm/\"\n",
    "DISPATCH_FORMAT = \"CWC_discharge_{0}_clean\"\n",
    "PATH_CATCHMENTS = PATH_ROOT + \"Data/catchments.xlsx\"\n",
    "FILE_FORMAT = \"data_{0}_{1}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lat - width, Lon - height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAT_MIN = 17.375\n",
    "LAT_MAX = 22.625\n",
    "LON_MIN = 73.625\n",
    "LON_MAX = 82.875\n",
    "GRID_DELTA = 0.25\n",
    "LAT_GRID = np.arange(LAT_MIN, LAT_MAX + GRID_DELTA / 2, GRID_DELTA)\n",
    "LON_GRID = np.arange(LON_MIN, LON_MAX + GRID_DELTA / 2, GRID_DELTA)\n",
    "DATA_LEN = 17532\n",
    "NUM_CHANNELS = 3\n",
    "DEFAULT_LAT = len(LAT_GRID)\n",
    "DEFAULT_LON = len(LON_GRID)\n",
    "DATA_START_DATE = (1967, 1, 1)\n",
    "DATA_END_DATE = (2014, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(device, model, optimizer, loader, loss_func, epoch):\n",
    "    \"\"\"Train model for a single epoch.\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param optimizer: One of PyTorchs optimizer classes.\n",
    "    :param loader: A PyTorch DataLoader, providing the trainings\n",
    "        data in mini batches.\n",
    "    :param loss_func: The loss function to minimize.\n",
    "    :param epoch: The current epoch (int) used for the progress bar\n",
    "    \"\"\"\n",
    "    # set model to train mode (important for dropout)\n",
    "    model.train()\n",
    "    pbar = tqdm.notebook.tqdm(loader)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    # request mini-batch of data from the loader\n",
    "    for xs, ys in pbar:\n",
    "        # delete previously stored gradients from the model\n",
    "        optimizer.zero_grad()\n",
    "        # push data to GPU (if available)\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "        # get model predictions\n",
    "        y_hat = model(xs)\n",
    "        # calculate loss\n",
    "        loss = loss_func(y_hat, ys)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # write current loss in the progress bar\n",
    "        pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(device, model, loader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Evaluate the model.\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param loader: A PyTorch DataLoader, providing the data.\n",
    "    :return: Two torch Tensors, containing the observations and\n",
    "        model predictions\n",
    "    \"\"\"\n",
    "    # set model to eval mode (important for dropout)\n",
    "    model.eval()\n",
    "    obs = []\n",
    "    preds = []\n",
    "    # in inference mode, we don't need to store intermediate steps for\n",
    "    # backprob\n",
    "    with torch.no_grad():\n",
    "        # request mini-batch of data from the loader\n",
    "        for xs, ys in loader:\n",
    "            # push data to GPU (if available)\n",
    "            xs = xs.to(device)\n",
    "            # get model predictions\n",
    "            y_hat = model(xs)\n",
    "            obs.append(ys)\n",
    "            preds.append(y_hat)\n",
    "    return torch.cat(obs), torch.cat(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nse(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    # only consider time steps, where observations are available\n",
    "    # COMMENT FROM EFRAT TO RONEN: NEGATIVE VALUES ARE FINE! I COMMENTED THE TWO LINES BELOW\n",
    "    # sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n",
    "    # obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n\n",
    "    # check for NaNs in observations\n",
    "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    nse_val = 1 - numerator / denominator\n",
    "    return nse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_persist_nse(obs: np.array, sim: np.array, lead) -> float:\n",
    "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    # only consider time steps, where observations are available\n",
    "    # COMMENT FROM EFRAT TO RONEN: NEGATIVE VALUES ARE FINE! I COMMENTED THE TWO LINES BELOW\n",
    "    # sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n",
    "    # obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n\n",
    "    # check for NaNs in observations\n",
    "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n\n",
    "    # the reference is the last observed, instead of the mean\n",
    "    sim = sim[lead:]\n",
    "    obs = obs[lead:]\n",
    "    ref = obs[:-lead]\n",
    "    denominator = np.sum((obs - ref) ** 2)\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    persist_nse_val = 1 - numerator / denominator\n",
    "    return persist_nse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bias(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\" Calculate bias\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    bias_95 = None\n",
    "    if (np.percentile(obs, 95) != 0):\n",
    "        bias_95 = (np.percentile(sim, 95) - np.percentile(obs, 95)) / np.percentile(obs, 95) * 100\n",
    "    bias_5 = None\n",
    "    if (np.percentile(obs, 5) != 0):\n",
    "        bias_5 = (np.percentile(sim, 5) - np.percentile(obs, 5)) / np.percentile(obs, 5) * 100\n",
    "    mean_bias = None\n",
    "    if (np.nanmean(obs) != 0):\n",
    "        mean_bias = (np.nanmean(sim) - np.nanmean(obs)) / np.nanmean(obs) * 100\n",
    "    return bias_95, bias_5, mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_maxdif(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\" Calculate max difference in percent\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: maxdif value.\n",
    "    \"\"\"\n",
    "    max_sim = np.nanmax(sim)\n",
    "    max_obs = np.nanmax(obs)\n",
    "    return (max_sim - max_obs) / max_obs * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vol_qp(obs: np.array) -> float:\n",
    "    \"\"\" Calculate volume [10^6 m^3] and peak discharge [m^3/s]\n",
    "    :param obs: Array containing the observations\n",
    "    :return: vol and qp values.\n",
    "    \"\"\"\n",
    "    vol = np.nansum(obs) * 3600 * 24 / 1E6  # translate from m^3/s in daily resolution to 10^6 m^3\n",
    "    qp = np.nanmax(obs)\n",
    "    return vol, qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_number(number):\n",
    "    try:\n",
    "        ret_number = int(number)\n",
    "        return ret_number\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Whether to use CPU or GPU. Use False for CPU mode.\n",
    "    use_gpu = True\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and use_gpu) else \"cpu\")\n",
    "    # Set random seed for reproducibility\n",
    "    # manualSeed = 999\n",
    "    # #manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "    # #print(\"Random Seed: \", manualSeed)\n",
    "    # random.seed(manualSeed)\n",
    "    # torch.manual_seed(manualSeed)\n",
    "    #################################\n",
    "    ###### Meta parameters ##########\n",
    "    #################################\n",
    "    hidden_size = 128  # Number of LSTM cells\n",
    "    dropout_rate = 0.01  # Dropout rate of the final fully connected Layer [0.0, 1.0]\n",
    "    # learning_rate = 2e-3 # Learning rate used to update the weights\n",
    "    learning_rate = 1e-4  # Learning rate used to update the weights\n",
    "    sequence_length = 30  # Length of the meteorological record provided to the network\n",
    "    num_layers = 2  # Number of LSTM cells\n",
    "    lead = 0  # 1\n",
    "    cnn_outputsize = 20\n",
    "    num_hidden_layers = 3\n",
    "    num_hidden_units = 128\n",
    "    ### Choose features ###\n",
    "    use_perc = True\n",
    "    # maximum temprature in a given day\n",
    "    use_t_max = False\n",
    "    # minimum temprature in a given day\n",
    "    use_t_min = False\n",
    "    idx_features = [use_perc, use_t_max, use_t_min]\n",
    "    ### Choose basin ###\n",
    "    # basin_list = ['Mancherial', 'Perur' ,'Pathagudem','Polavaram', 'Tekra']\n",
    "    basin_list = ['Tekra', 'Perur']\n",
    "    preprocessor = Preprocessor(PATH_ROOT, idx_features, DATA_START_DATE, DATA_END_DATE, LAT_MIN,\n",
    "                                LAT_MAX, LON_MIN, LON_MAX, GRID_DELTA, DATA_LEN, NUM_CHANNELS)\n\n",
    "    # The data will always be in shape of - samples * channels * width * height\n",
    "    all_data, image_width, image_height = preprocessor.reshape_data_by_lat_lon_file(\n",
    "        PATH_DATA_FILE, DIMS_JSON_FILE_PATH)\n\n",
    "    ##############\n",
    "    # Data set up#\n",
    "    ##############\n",
    "    catchment_dict = preprocessor.create_catchment_dict(PATH_CATCHMENTS)\n",
    "    include_static = True\n\n",
    "    # Training data\n",
    "    start_date = (2000, 1, 1)\n",
    "    end_date = (2009, 12, 31)\n",
    "    months_lst = [6, 7, 8, 9, 10]\n",
    "    print('Train dataset\\n===============================')\n",
    "    ds_train = IMDGodavari(all_data,\n",
    "                           catchment_dict=catchment_dict,\n",
    "                           preprocessor=preprocessor,\n",
    "                           basin_list=basin_list,\n",
    "                           seq_length=sequence_length,\n",
    "                           period=\"train\",\n",
    "                           dates=[start_date, end_date],\n",
    "                           months=months_lst,\n",
    "                           idx=idx_features,\n",
    "                           lead=lead,\n",
    "                           include_static=include_static)\n",
    "    tr_loader = DataLoader(ds_train, batch_size=64, shuffle=True)\n\n",
    "    # Test data. We use the feature min/max of the training period for normalization\n",
    "    start_date = (2000, 1, 1)\n",
    "    end_date = (2009, 12, 31)\n",
    "    print('\\nTest dataset\\n===============================')\n",
    "    ds_test = IMDGodavari(all_data,\n",
    "                          basin_list,\n",
    "                          catchment_dict=catchment_dict,\n",
    "                          preprocessor=preprocessor,\n",
    "                          seq_length=sequence_length,\n",
    "                          period=\"eval\",\n",
    "                          dates=[start_date, end_date],\n",
    "                          months=months_lst,\n",
    "                          idx=idx_features, lead=lead,\n",
    "                          min_values=ds_train.get_min(),\n",
    "                          max_values=ds_train.get_max(),\n",
    "                          mean_y=ds_train.get_mean_y(),\n",
    "                          std_y=ds_train.get_std_y(),\n",
    "                          include_static=include_static)\n",
    "    test_loader = DataLoader(ds_test, batch_size=2048, shuffle=False)\n\n",
    "    #########################\n",
    "    # Model, Optimizer, Loss#\n",
    "    #########################\n\n",
    "    # Here we create our model\n",
    "    # attributes == static features\n",
    "    num_attributes = catchment_dict['Tekra'].size\n",
    "    if not include_static:\n",
    "        num_attributes = 0\n\n",
    "    # idx_features - a True / False list over the 3 features (channels) of each \"image\"\n",
    "    input_size = (sum(idx_features) * DEFAULT_LON * DEFAULT_LAT + num_attributes) * sequence_length\n",
    "    input_image_size = (sum(idx_features), image_width, image_height)\n",
    "    model = CNNLSTM(lat=image_width, lon=image_height, input_size=cnn_outputsize, num_layers=num_layers,\n",
    "                    hidden_size=hidden_size,\n",
    "                    dropout_rate=dropout_rate, num_channels=sum(idx_features),\n",
    "                    num_attributes=num_attributes, image_input_size=input_image_size).to(device)\n",
    "    # model = DNN(input_size=input_size, num_hidden_layers=num_hidden_layers,\n",
    "    # num_hidden_units=num_hidden_units,\n",
    "    # dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.MSELoss()\n",
    "    n_epochs = 50  # Number of training epochs\n\n",
    "    # Creating the checkpoint folders\n",
    "    datetime_israel = datetime.datetime.now(pytz.timezone('Israel'))\n",
    "    path_train_ckpt = PATH_MODEL + datetime_israel.strftime(\"%Y_%m_%d-%H-%M-%S/\")\n",
    "    pathlib.Path(path_train_ckpt).mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(n_epochs):\n",
    "        train_epoch(device, model, optimizer, tr_loader, loss_func, i + 1)\n",
    "        obs, preds = eval_model(device, model, test_loader)\n",
    "        preds = ds_test.local_rescale(preds.cpu().numpy())\n",
    "        nse = calc_nse(obs.numpy(), preds)\n",
    "        tqdm.tqdm.write(f\"Test NSE: {nse:.3f}\")\n",
    "        model_name = \"epoch_{:d}_nse_{:.3f}.ckpt\".format(i + 1, nse)\n",
    "        torch.save(model, path_train_ckpt + model_name)\n",
    "        last_model_path = path_train_ckpt + model_name\n\n",
    "    # start_date = (2000, 1, 1)\n",
    "    # end_date = (2014, 12, 31)\n",
    "    start_date = (2010, 1, 1)\n",
    "    end_date = (2014, 12, 31)\n",
    "    months_lst = [6, 7, 8, 9, 10]\n",
    "    Validation_basin = [\"Tekra\"]\n",
    "    ds_val = IMDGodavari(all_data,\n",
    "                         basin_list=Validation_basin,\n",
    "                         catchment_dict=catchment_dict,\n",
    "                         preprocessor=preprocessor,\n",
    "                         seq_length=sequence_length,\n",
    "                         period=\"eval\",\n",
    "                         dates=[start_date, end_date],\n",
    "                         months=months_lst,\n",
    "                         idx=idx_features,\n",
    "                         lead=lead,\n",
    "                         min_values=ds_train.get_min(),\n",
    "                         max_values=ds_train.get_max(),\n",
    "                         mean_y=ds_train.get_mean_y(),\n",
    "                         std_y=ds_train.get_std_y(),\n",
    "                         include_static=include_static)\n",
    "    val_loader = DataLoader(ds_val, batch_size=2048, shuffle=False)\n",
    "    obs, preds = eval_model(device, model, val_loader)\n",
    "    preds = ds_val.local_rescale(preds.cpu().numpy())\n",
    "    obs = obs.numpy()\n",
    "    nse = calc_nse(obs, preds)\n",
    "    pb95, pb5, total_b = calc_bias(obs, preds)\n",
    "    # Plot results\n",
    "    start_date_tpl = ds_val.dates[0]\n",
    "    start_date = pd.to_datetime(\n",
    "        datetime.datetime(start_date_tpl[0], start_date_tpl[1], start_date_tpl[2], 0, 0)) + pd.DateOffset(\n",
    "    )\n",
    "    end_date_tpl = ds_val.dates[1]\n",
    "    temp = pd.to_datetime(datetime.datetime(end_date_tpl[0], end_date_tpl[1], end_date_tpl[2], 0, 0))\n",
    "    end_date = temp + pd.DateOffset()\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    # months = get_months_by_dates(start_date, end_date)\n",
    "    ind_include = [i for i in range(0, len(date_range)) if date_range[i].month in months_lst]\n",
    "    date_range = date_range[ind_include]\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    ax.plot(date_range, obs, label=\"observation\")\n",
    "    ax.plot(date_range, preds, label=\"prediction\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\n",
    "        f\"Basin {Validation_basin} - Validation set NSE: {nse:.3f}, \"\n",
    "        f\"95bias: {pb95:.1f}, 5bias: {pb5:.3f} ,total bias : {total_b: .1f}%\")\n",
    "    ax.xaxis.set_tick_params(rotation=90)\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.grid('on')\n",
    "    _ = ax.set_ylabel(\"Discharge (mm/d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n# Integrated gradients\n<br>\n",
    "# # Calculate Integrated Gradients<br>\n",
    "#<br>\n",
    "# start_date_ig = (2012, 8, 26)<br>\n",
    "# end_date_ig = (2012, 9, 5)<br>\n",
    "# model.eval()<br>\n",
    "# model.cpu()<br>\n",
    "# ig = IntegratedGradients(model, multiply_by_inputs=True)<br>\n",
    "# basline = torch.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n",
    "# integ_grad = np.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n",
    "# for i in idx:<br>\n",
    "#     integ_grad += ig.attribute(ds_val.x[i:(i + 1), :, :], basline).numpy()<br>\n",
    "# integ_grad = np.squeeze(integ_grad)<br>\n",
    "# integ_grad /= len(idx)<br>\n",
    "# _ = model.cuda()<br>\n",
    "#<br>\n",
    "# image_grad = integ_grad[:, :DEFAULT_LAT * DEFAULT_LON].reshape((sequence_length, DEFAULT_LAT, DEFAULT_LON))<br>\n",
    "# time_vector_grad = np.sum(image_grad.reshape((image_grad.shape[0], image_grad.shape[1] * image_grad.shape[2])), axis=1)<br>\n",
    "# spatial_image_grad = np.sum(image_grad, axis=0)<br>\n",
    "# atrrib_grade = integ_grad[:, DEFAULT_LAT * DEFAULT_LON:]<br>\n",
    "#<br>\n",
    "# predsmonsoon = preds[np.where((date_range.month >= 6) & (date_range.month <= 10))[0]]<br>\n",
    "# obsmonsoon = obs[np.where((date_range.month >= 6) & (date_range.month <= 10))[0]]<br>\n",
    "# threshq1 = np.percentile(predsmonsoon, 90)<br>\n",
    "# threshq2 = np.percentile(predsmonsoon, 55)<br>\n",
    "# # idx = np.asarray([i for i in range(0,len(preds)) if (preds[i]>threshq1) & (preds[i]<threshq2)])<br>\n",
    "# idx = np.asarray([i for i in range(0, len(preds)) if (preds[i] > threshq1)])<br>\n",
    "# # idx = np.where((preds>threshq1) & (preds<threshq2) & (date_range.month>=6) & (date_range.month<=10))[0]<br>\n",
    "# print([threshq1, threshq2, idx.shape])<br>\n",
    "# # set model to eval mode (important for dropout)<br>\n",
    "# model.eval()<br>\n",
    "# model.cpu()<br>\n",
    "# ig = IntegratedGradients(model, multiply_by_inputs=True)<br>\n",
    "# basline = torch.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n",
    "# integ_grad = np.zeros(ds_val.x[idx[0]:idx[0] + 1, :, :].shape)<br>\n",
    "# for i in idx:<br>\n",
    "#     # print (i)<br>\n",
    "#     integ_grad += ig.attribute(ds_val.x[i:(i + 1), :, :], basline).numpy()<br>\n",
    "# integ_grad = np.squeeze(integ_grad)<br>\n",
    "# integ_grad /= len(idx)<br>\n",
    "# _ = model.cuda()<br>\n",
    "#<br>\n",
    "# image_grad = integ_grad[:, :DEFAULT_LAT * DEFAULT_LON].reshape((sequence_length, DEFAULT_LAT, DEFAULT_LON))<br>\n",
    "# time_vector_grad = np.sum(image_grad.reshape((image_grad.shape[0], image_grad.shape[1] * image_grad.shape[2])), axis=1)<br>\n",
    "# spatial_image_grad = np.sum(image_grad, axis=0)<br>\n",
    "# atrrib_grade = integ_grad[:, DEFAULT_LAT * DEFAULT_LON:]<br>\n",
    "#<br>\n",
    "# # integ_file = PATH_ROOT + \"Out/integ_grad_2000_2014\"<br>\n",
    "# # np.save(file=integ_file, arr=integ_grad)<br>\n",
    "#<br>\n",
    "# # Plot Integrated Gradients - Spatial<br>\n",
    "# sequence_length_small = 9<br>\n",
    "# image_grad_small = image_grad[sequence_length - sequence_length_small:, :]<br>\n",
    "# n_w_win = 3<br>\n",
    "# n_h_win = int((sequence_length_small + 1) / n_w_win)<br>\n",
    "# fig, ax = plt.subplots(n_h_win, n_w_win, figsize=(10 * n_h_win, 6 * n_w_win))<br>\n",
    "# max_v = abs(image_grad_small).max()<br>\n",
    "# min_v = -max_v<br>\n",
    "# for i in range(sequence_length_small):<br>\n",
    "#     ax.flat[i].set_title(f'Day {i - sequence_length_small}')<br>\n",
    "#     df = pd.DataFrame(image_grad_small[i, :], index=list(LAT_GRID), columns=list(LON_GRID))<br>\n",
    "#     sns.heatmap(df[::-1], ax=ax.flat[i], vmin=min_v, vmax=max_v, square=True, cmap='RdYlBu')<br>\n",
    "#<br>\n",
    "# # plot without catchment attributes<br>\n",
    "# with plt.style.context('ggplot'):<br>\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))<br>\n",
    "#     # ax1.plot(new_date_range, obs[idx], label=\"observation\")<br>\n",
    "#     # ax1.plot(new_date_range, preds[idx], label=\"prediction\")<br>\n",
    "#     # ax1.plot(idx, obs[idx], 'x', label=\"observation\")<br>\n",
    "#     # ax1.plot(idx, preds[idx],'x', label=\"prediction\")<br>\n",
    "#     ax1.plot(idx, obs[idx], 'x', label=\"observation\")<br>\n",
    "#     ax1.plot(idx, preds[idx], 'x', label=\"prediction\")<br>\n",
    "#     ax1.legend()<br>\n",
    "#     #  ax1.set_title(f\"Basin {Validation_basin} discharge>q95\")<br>\n",
    "#     ax1.set_title(f\"Basin {Validation_basin}, monsoon, discharge: >q90\")<br>\n",
    "#     # ax1.xaxis.set_tick_params(rotation=90)<br>\n",
    "#     ax1.set_xlabel(\"Date\")<br>\n",
    "#     ax1.grid('on')<br>\n",
    "#     _ = ax1.set_ylabel(\"Discharge (mm/d)\")<br>\n",
    "#<br>\n",
    "#     #  df = pd.DataFrame(spatial_image_grad, index =list(LAT_GRID), columns =list(LON_GRID))<br>\n",
    "#     df = pd.DataFrame(image_grad_small[sequence_length_small - 3, :], index=list(LAT_GRID), columns=list(LON_GRID))<br>\n",
    "#     #  vmax=np.max(np.abs(spatial_image_grad.flat))<br>\n",
    "#     vmax = np.max(np.abs(image_grad_small[sequence_length_small - 3, :].flat))<br>\n",
    "#     sns.heatmap(df[::-1], ax=ax2, square=True, cmap='RdYlBu', vmin=-vmax, vmax=vmax)<br>\n",
    "#<br>\n",
    "#     txn = np.arange(-sequence_length + 1, 0 + 1)<br>\n",
    "#     ax3.plot(txn, time_vector_grad, '-o', color='c')<br>\n",
    "#     ax3.set_xlabel(\"Day\")<br>\n",
    "#     ax3.set_ylabel(\"Integrated gradients\")<br>\n",
    "#     ax3.set_xlim([-sequence_length + 1, 0])<br>\n",
    "#     ax3.set_xticks(txn)<br>\n",
    "#<br>\n",
    "# # plot<br>\n",
    "# with plt.style.context('ggplot'):<br>\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))<br>\n",
    "#     ax1.plot(new_date_range, obs[idx], label=\"observation\")<br>\n",
    "#     ax1.plot(new_date_range, preds[idx], label=\"prediction\")<br>\n",
    "#     ax1.legend()<br>\n",
    "#     ax1.set_title(f\"Basin {Validation_basin}\")<br>\n",
    "#     # ax1.xaxis.set_tick_params(rotation=90)<br>\n",
    "#     ax1.set_xlabel(\"Date\")<br>\n",
    "#     ax1.grid('on')<br>\n",
    "#     _ = ax1.set_ylabel(\"Discharge (mm/d)\")<br>\n",
    "#<br>\n",
    "#     ax2.bar(['Precipitation', \"Mean Precipitation\", \"Aridity\", \"Area\", \"Mean elevation\"], sum(att),<br>\n",
    "#             color=['b', 'g', 'g', 'g', 'g'])<br>\n",
    "#     ax2.plot(['Precipitation', \"Mean Precipitation\", \"Aridity\", \"Area\", \"Mean elevation\"], [0, 0, 0, 0, 0], 'k')<br>\n",
    "#     ax2.set_ylabel(\"Attribute sum integrated gradients\")<br>\n",
    "#<br>\n",
    "#     txn = np.arange(-sequence_length + 1, 0 + 1)<br>\n",
    "#     ax3.plot(txn, att[:, 0], '-o', color='c')<br>\n",
    "#     ax3.set_xlabel(\"Day\")<br>\n",
    "#     ax3.set_ylabel(\"Integrated gradients\")<br>\n",
    "#     ax3.set_xlim([-sequence_length + 1, 0])<br>\n",
    "#     ax3.set_xticks(txn)<br>\n",
    "#<br>\n",
    "# # This cell is for creating the raw data - no need to run this<br>\n",
    "# start_date_pd = pd.to_datetime(datetime.datetime(DATA_START_DATE[0], DATA_START_DATE[1], DATA_START_DATE[2], 0, 0))<br>\n",
    "# end_date_pd = pd.to_datetime(datetime.datetime(DATA_END_DATE[0], DATA_END_DATE[1], DATA_END_DATE[2], 0, 0))<br>\n",
    "# date_range = pd.date_range(start_date_pd, end_date_pd)<br>\n",
    "# num_days = len(date_range)<br>\n",
    "# num_features = 3<br>\n",
    "# h = len(LAT_GRID)<br>\n",
    "# w = len(LON_GRID)<br>\n",
    "# data = np.zeros((num_days, num_features, h, w))<br>\n",
    "# for i, lat_i in enumerate(LAT_GRID):<br>\n",
    "#     for j, lon_j in enumerate(LON_GRID):<br>\n",
    "#         x = get_geo_raw_data(lat_i, lon_j, start_date, end_date)<br>\n",
    "#         data[:, :, i, j] = x<br>\n",
    "# out_path = PATH_ROOT + 'Data/'<br>\n",
    "# data.tofile(out_path + \"raw_data_fixed\" + '_'.join([str(_) for _ in data.shape]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}